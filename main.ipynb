{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1418575a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import os\n",
    "import json\n",
    "\n",
    "def download_from_arxiv(key_word, max_results=10, destination_path=\"assets/arxiv/\", saved_list_path=\"assets/arxiv/papers.json\"):\n",
    "    client = arxiv.Client()\n",
    "\n",
    "    search = arxiv.Search(\n",
    "        query = str(key_word),\n",
    "        max_results = max_results,\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate, \n",
    "        sort_order=arxiv.SortOrder.Descending\n",
    "    )\n",
    "    # Проверка существования файла с сохраненным списком скачанных arXiv ID\n",
    "    if os.path.exists(saved_list_path):\n",
    "        with open(saved_list_path, \"r\") as f:\n",
    "            downloaded_arxiv_ids = json.load(f)\n",
    "    else:\n",
    "        downloaded_arxiv_ids = []\n",
    "    \n",
    "    for result in client.results(search):\n",
    "        # Проверка наличия статьи в списке уже скачанных\n",
    "        if result.entry_id.split(\"/\")[-1] in downloaded_arxiv_ids:\n",
    "            print(f\"Статья {result.entry_id} уже скачана и пропущена.\")\n",
    "            continue\n",
    "        \n",
    "        # Скачивание PDF-файла статьи\n",
    "        pdf_path = result.download_pdf(dirpath=destination_path)\n",
    "        if pdf_path:\n",
    "            print(f\"Статья {result.entry_id} успешно скачана и сохранена в {pdf_path}\")\n",
    "            # Добавление arXiv ID в список скачанных\n",
    "            downloaded_arxiv_ids.append(result.entry_id.split(\"/\")[-1])\n",
    "    # Сохранение списка скачанных arXiv ID в файл JSON\n",
    "    with open(saved_list_path, \"w\") as f:\n",
    "        json.dump(downloaded_arxiv_ids, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "01a665de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статья http://arxiv.org/abs/2404.15238v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15104v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.14977v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.14963v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.14943v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15382v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.14809v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.14740v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.14695v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.14631v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15488v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.14043v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.13948v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.13892v2 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.13781v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.12879v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.12772v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.12560v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.12457v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.12309v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.14901v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.13957v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.13764v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.13633v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.12926v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.12309v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.09577v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15351v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.09375v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.13066v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15604v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15592v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15588v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15578v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15549v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15522v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15515v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15488v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15485v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15458v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15615v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15608v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15604v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15603v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15592v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15591v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15580v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15576v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15564v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15552v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.06751v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2403.15729v2 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2403.14258v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2403.14702v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2403.10482v2 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2403.10588v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2403.00830v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2403.05568v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2402.06929v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2402.01733v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15617v1 успешно скачана и сохранена в assets/arxiv/2404.15617v1.DPO__Differential_reinforcement_learning_with_application_to_optimal_configuration_search.pdf\n",
      "Статья http://arxiv.org/abs/2404.15607v1 успешно скачана и сохранена в assets/arxiv/2404.15607v1.A_Note_on_Approximating_Weighted_Nash_Social_Welfare_with_Additive_Valuations.pdf\n",
      "Статья http://arxiv.org/abs/2404.15604v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15602v1 успешно скачана и сохранена в assets/arxiv/2404.15602v1.Decentralized_Multi_Agent_Trajectory_Planning_in_Dynamic_Environments_with_Spatiotemporal_Occupancy_Grid_Maps.pdf\n",
      "Статья http://arxiv.org/abs/2404.15597v1 успешно скачана и сохранена в assets/arxiv/2404.15597v1.GRSN__Gated_Recurrent_Spiking_Neurons_for_POMDPs_and_MARL.pdf\n",
      "Статья http://arxiv.org/abs/2404.15592v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15588v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15583v1 успешно скачана и сохранена в assets/arxiv/2404.15583v1.Multi_Agent_Reinforcement_Learning_for_Energy_Networks__Computational_Challenges__Progress_and_Open_Problems.pdf\n",
      "Статья http://arxiv.org/abs/2404.15581v1 успешно скачана и сохранена в assets/arxiv/2404.15581v1.Decentralized_Exchangeable_Stochastic_Dynamic_Teams_in_Continuous_time__their_Mean_Field_Limits_and_Optimality_of_Symmetric_Policies.pdf\n",
      "Статья http://arxiv.org/abs/2404.15578v1 уже скачана и пропущена.\n"
     ]
    }
   ],
   "source": [
    "key_words = [\"NLP\", \"RAG\", \"ChatBot\", \"LLM\", \"Speech Recognition\", \"LangChain\", \"LLM Agents\"]\n",
    "\n",
    "for key_word in key_words:\n",
    "    # Пример использования функции\n",
    "    download_from_arxiv(key_word, max_results=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8cc70617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9decbb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Coroutine, List\n",
    "\n",
    "\n",
    "class HuggingFaceE5Embeddings(HuggingFaceEmbeddings):\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        text = f\"query: {text}\"\n",
    "        return super().embed_query(text)\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        texts = [f\"passage: {text}\" for text in texts]\n",
    "        return super().embed_documents(texts)\n",
    "\n",
    "    async def aembed_query(self, text: str) -> Coroutine[Any, Any, List[float]]:\n",
    "        text = f\"query: {text}\"\n",
    "        return await super().aembed_query(text)\n",
    "\n",
    "    async def aembed_documents(\n",
    "        self, texts: List[str]\n",
    "    ) -> Coroutine[Any, Any, List[List[float]]]:\n",
    "        texts = [f\"passage: {text}\" for text in texts]\n",
    "        return await super().aembed_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4ebfeb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = HuggingFaceE5Embeddings(model_name=\"intfloat/multilingual-e5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ee482890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 66 0 (offset 0)\n",
      "Ignoring wrong pointing object 68 0 (offset 0)\n",
      "Ignoring wrong pointing object 80 0 (offset 0)\n",
      "Ignoring wrong pointing object 85 0 (offset 0)\n",
      "Ignoring wrong pointing object 232 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Ignoring wrong pointing object 34 0 (offset 0)\n",
      "Ignoring wrong pointing object 44 0 (offset 0)\n",
      "Ignoring wrong pointing object 46 0 (offset 0)\n",
      "Ignoring wrong pointing object 55 0 (offset 0)\n",
      "Ignoring wrong pointing object 57 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain_community.embeddings.gigachat import GigaChatEmbeddings\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "# embeddings = GigaChatEmbeddings(\n",
    "#     credentials=credentials, scope=\"GIGACHAT_API_CORP\", verify_ssl_certs=True\n",
    "# )\n",
    "\n",
    "loader = PyPDFDirectoryLoader(\"assets/arxiv/\")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=10000,\n",
    "    chunk_overlap=3000,\n",
    ")\n",
    "documents = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "06a540d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_db = FAISS.from_documents(documents, embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6ba94b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def tokenize(s: str) -> list[str]:\n",
    "    \"\"\"Очень простая функция разбития предложения на слова\"\"\"\n",
    "    return s.lower().translate(str.maketrans(\"\", \"\", string.punctuation)).split(\" \")\n",
    "\n",
    "embedding_retriever = faiss_db.as_retriever(search_kwargs={\"k\": 2})\n",
    "bm25_retriever = BM25Retriever.from_documents(\n",
    "    documents=documents,\n",
    "    preprocess_func=tokenize,\n",
    "    k=3,\n",
    ")\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[embedding_retriever, bm25_retriever],\n",
    "    weights=[0.4, 0.6],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "1341a526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': \"Придумай 4 похожих термина ['Natural Language Processing', \"\n",
      "          \"'Retrieval-Augmented Generation']. Не повторяйся\",\n",
      " 'result': '1. Natural Language Processing (NLP)\\n'\n",
      "           '2. Machine Translation (MT)\\n'\n",
      "           '3. Information Retrieval (IR)\\n'\n",
      "           '4. Text Mining (TM)'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms.gigachat import GigaChat\n",
    "from pprint import pprint\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "llm = GigaChat(credentials=credentials, \n",
    "               scope=\"GIGACHAT_API_CORP\", \n",
    "               verify_ssl_certs=False, \n",
    "               temperature=0.5,\n",
    "               max_tokens=200)\n",
    "key_words = ['Natural Language Processing', 'Retrieval-Augmented Generation']\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=ensemble_retriever\n",
    ")\n",
    "pprint(qa.invoke(f\"Придумай 4 похожих термина {key_words}. Не повторяйся\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e090fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(qa.invoke(\"Что такое RAD. Напиши пример кода\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a64317d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RetrievalQA(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"), llm=GigaChat(credentials='YmZlNGMwYWQtM2E0ZS00NzQ3LWIzMzQtZWYxN2NjNTYxODEyOmIyMjkxZTI2LTg4NjktNDc1Yy05NjE5LTg2NzUxNzc3MWZmYg==', scope='GIGACHAT_API_CORP', verify_ssl_certs=False, temperature=1.5, _client=<gigachat.GigaChat object at 0x7fb9119ed8d0>)), document_variable_name='context'), retriever=EnsembleRetriever(retrievers=[VectorStoreRetriever(tags=['FAISS', 'HuggingFaceE5Embeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7fb8bffe93c0>, search_kwargs={'k': 2}), BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x7fb8db1c19f0>, k=3, preprocess_func=<function tokenize at 0x7fbab4c817e0>)], weights=[0.4, 0.6]))\n"
     ]
    }
   ],
   "source": [
    "pprint(qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5558cb7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "giga_kernel",
   "language": "python",
   "name": "giga_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
