{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b5a7204c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feed title: ArXiv Query: search_query=all:rag&amp;id_list=&amp;start=0&amp;max_results=5\n",
      "Feed last updated: 2024-04-24T00:00:00-04:00\n",
      "totalResults for this query: 295\n",
      "itemsPerPage for this query: 5\n",
      "startIndex for this query: 0\n",
      "e-print metadata\n",
      "arxiv-id: 2404.14043v1\n",
      "Published: 2024-04-22T09:56:59Z\n",
      "Title:  LLMs Know What They Need: Leveraging a Missing Information Guided\n",
      "  Framework to Empower Retrieval-Augmented Generation\n",
      "Authors:  Keheng Wang,Feiyu Duan,Peiguang Li,Sirui Wang,Xunliang Cai\n",
      "abs page link: http://arxiv.org/abs/2404.14043v1\n",
      "pdf link: http://arxiv.org/pdf/2404.14043v1\n",
      "Journal reference: No journal ref found\n",
      "Comments: No comment found\n",
      "Primary Category: cs.CL\n",
      "All Categories: cs.CL\n",
      "Abstract: Retrieval-Augmented Generation (RAG) demonstrates great value in alleviating\n",
      "outdated knowledge or hallucination by supplying LLMs with updated and relevant\n",
      "knowledge. However, there are still several difficulties for RAG in\n",
      "understanding complex multi-hop query and retrieving relevant documents, which\n",
      "require LLMs to perform reasoning and retrieve step by step. Inspired by\n",
      "human's reasoning process in which they gradually search for the required\n",
      "information, it is natural to ask whether the LLMs could notice the missing\n",
      "information in each reasoning step. In this work, we first experimentally\n",
      "verified the ability of LLMs to extract information as well as to know the\n",
      "missing. Based on the above discovery, we propose a Missing Information Guided\n",
      "Retrieve-Extraction-Solving paradigm (MIGRES), where we leverage the\n",
      "identification of missing information to generate a targeted query that steers\n",
      "the subsequent knowledge retrieval. Besides, we design a sentence-level\n",
      "re-ranking filtering approach to filter the irrelevant content out from\n",
      "document, along with the information extraction capability of LLMs to extract\n",
      "useful information from cleaned-up documents, which in turn to bolster the\n",
      "overall efficacy of RAG. Extensive experiments conducted on multiple public\n",
      "datasets reveal the superiority of the proposed MIGRES method, and analytical\n",
      "experiments demonstrate the effectiveness of our proposed modules.\n",
      "\n",
      "e-print metadata\n",
      "arxiv-id: 2404.13948v1\n",
      "Published: 2024-04-22T07:49:36Z\n",
      "Title:  Typos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by\n",
      "  Simulating Documents in the Wild via Low-level Perturbations\n",
      "Authors:  Sukmin Cho,Soyeong Jeong,Jeongyeon Seo,Taeho Hwang,Jong C. Park\n",
      "abs page link: http://arxiv.org/abs/2404.13948v1\n",
      "pdf link: http://arxiv.org/pdf/2404.13948v1\n",
      "Journal reference: No journal ref found\n",
      "Comments: Under Review\n",
      "Primary Category: cs.CL\n",
      "All Categories: cs.CL\n",
      "Abstract: The robustness of recent Large Language Models (LLMs) has become increasingly\n",
      "crucial as their applicability expands across various domains and real-world\n",
      "applications. Retrieval-Augmented Generation (RAG) is a promising solution for\n",
      "addressing the limitations of LLMs, yet existing studies on the robustness of\n",
      "RAG often overlook the interconnected relationships between RAG components or\n",
      "the potential threats prevalent in real-world databases, such as minor textual\n",
      "errors. In this work, we investigate two underexplored aspects when assessing\n",
      "the robustness of RAG: 1) vulnerability to noisy documents through low-level\n",
      "perturbations and 2) a holistic evaluation of RAG robustness. Furthermore, we\n",
      "introduce a novel attack method, the Genetic Attack on RAG (\\textit{GARAG}),\n",
      "which targets these aspects. Specifically, GARAG is designed to reveal\n",
      "vulnerabilities within each component and test the overall system functionality\n",
      "against noisy documents. We validate RAG robustness by applying our\n",
      "\\textit{GARAG} to standard QA datasets, incorporating diverse retrievers and\n",
      "LLMs. The experimental results show that GARAG consistently achieves high\n",
      "attack success rates. Also, it significantly devastates the performance of each\n",
      "component and their synergy, highlighting the substantial risk that minor\n",
      "textual inaccuracies pose in disrupting RAG systems in the real world.\n",
      "\n",
      "e-print metadata\n",
      "arxiv-id: 2404.13892v2\n",
      "Published: 2024-04-22T05:46:40Z\n",
      "Title:  Retrieval-Augmented Audio Deepfake Detection\n",
      "Authors:  Zuheng Kang,Yayun He,Botao Zhao,Xiaoyang Qu,Junqing Peng,Jing Xiao,Jianzong Wang\n",
      "abs page link: http://arxiv.org/abs/2404.13892v2\n",
      "pdf link: http://arxiv.org/pdf/2404.13892v2\n",
      "Journal reference: No journal ref found\n",
      "Comments: Accepted by the 2024 International Conference on Multimedia Retrieval\n",
      "  (ICMR 2024)\n",
      "Primary Category: cs.SD\n",
      "All Categories: cs.SD, cs.AI, eess.AS\n",
      "Abstract: With recent advances in speech synthesis including text-to-speech (TTS) and\n",
      "voice conversion (VC) systems enabling the generation of ultra-realistic audio\n",
      "deepfakes, there is growing concern about their potential misuse. However, most\n",
      "deepfake (DF) detection methods rely solely on the fuzzy knowledge learned by a\n",
      "single model, resulting in performance bottlenecks and transparency issues.\n",
      "Inspired by retrieval-augmented generation (RAG), we propose a\n",
      "retrieval-augmented detection (RAD) framework that augments test samples with\n",
      "similar retrieved samples for enhanced detection. We also extend the\n",
      "multi-fusion attentive classifier to integrate it with our proposed RAD\n",
      "framework. Extensive experiments show the superior performance of the proposed\n",
      "RAD framework over baseline methods, achieving state-of-the-art results on the\n",
      "ASVspoof 2021 DF set and competitive results on the 2019 and 2021 LA sets.\n",
      "Further sample analysis indicates that the retriever consistently retrieves\n",
      "samples mostly from the same speaker with acoustic characteristics highly\n",
      "consistent with the query audio, thereby improving detection performance.\n",
      "\n",
      "e-print metadata\n",
      "arxiv-id: 2404.13781v1\n",
      "Published: 2024-04-21T21:22:28Z\n",
      "Title:  Evaluating Retrieval Quality in Retrieval-Augmented Generation\n",
      "Authors:  Alireza Salemi,Hamed Zamani\n",
      "abs page link: http://arxiv.org/abs/2404.13781v1\n",
      "pdf link: http://arxiv.org/pdf/2404.13781v1\n",
      "Journal reference: No journal ref found\n",
      "Comments: No comment found\n",
      "Primary Category: cs.CL\n",
      "All Categories: cs.CL, cs.IR\n",
      "Abstract: Evaluating retrieval-augmented generation (RAG) presents challenges,\n",
      "particularly for retrieval models within these systems. Traditional end-to-end\n",
      "evaluation methods are computationally expensive. Furthermore, evaluation of\n",
      "the retrieval model's performance based on query-document relevance labels\n",
      "shows a small correlation with the RAG system's downstream performance. We\n",
      "propose a novel evaluation approach, eRAG, where each document in the retrieval\n",
      "list is individually utilized by the large language model within the RAG\n",
      "system. The output generated for each document is then evaluated based on the\n",
      "downstream task ground truth labels. In this manner, the downstream performance\n",
      "for each document serves as its relevance label. We employ various downstream\n",
      "task metrics to obtain document-level annotations and aggregate them using\n",
      "set-based or ranking metrics. Extensive experiments on a wide range of datasets\n",
      "demonstrate that eRAG achieves a higher correlation with downstream RAG\n",
      "performance compared to baseline methods, with improvements in Kendall's $\\tau$\n",
      "correlation ranging from 0.168 to 0.494. Additionally, eRAG offers significant\n",
      "computational advantages, improving runtime and consuming up to 50 times less\n",
      "GPU memory than end-to-end evaluation.\n",
      "\n",
      "e-print metadata\n",
      "arxiv-id: 2404.12879v1\n",
      "Published: 2024-04-19T13:27:38Z\n",
      "Title:  Unlocking Multi-View Insights in Knowledge-Dense Retrieval-Augmented\n",
      "  Generation\n",
      "Authors:  Guanhua Chen,Wenhan Yu,Lei Sha\n",
      "abs page link: http://arxiv.org/abs/2404.12879v1\n",
      "pdf link: http://arxiv.org/pdf/2404.12879v1\n",
      "Journal reference: No journal ref found\n",
      "Comments: No comment found\n",
      "Primary Category: cs.CL\n",
      "All Categories: cs.CL\n",
      "Abstract: While Retrieval-Augmented Generation (RAG) plays a crucial role in the\n",
      "application of Large Language Models (LLMs), existing retrieval methods in\n",
      "knowledge-dense domains like law and medicine still suffer from a lack of\n",
      "multi-perspective views, which are essential for improving interpretability and\n",
      "reliability. Previous research on multi-view retrieval often focused solely on\n",
      "different semantic forms of queries, neglecting the expression of specific\n",
      "domain knowledge perspectives. This paper introduces a novel multi-view RAG\n",
      "framework, MVRAG, tailored for knowledge-dense domains that utilizes\n",
      "intention-aware query rewriting from multiple domain viewpoints to enhance\n",
      "retrieval precision, thereby improving the effectiveness of the final\n",
      "inference. Experiments conducted on legal and medical case retrieval\n",
      "demonstrate significant improvements in recall and precision rates with our\n",
      "framework. Our multi-perspective retrieval approach unleashes the potential of\n",
      "multi-view information enhancing RAG tasks, accelerating the further\n",
      "application of LLMs in knowledge-intensive fields.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import feedparser\n",
    "\n",
    "# Base api query url\n",
    "base_url = \"http://export.arxiv.org/api/query?\"\n",
    "\n",
    "# Search parameters\n",
    "query_params = {\n",
    "    'search_query': \"all:rag\",\n",
    "    'start': 0,  # retreive the first 5 results\n",
    "    'max_results': 5,\n",
    "    'sortBy': \"submittedDate\",\n",
    "    'sortOrder': \"descending\"\n",
    "}\n",
    "query = urllib.parse.urlencode(query_params)\n",
    "\n",
    "# perform a GET request using the base_url and query\n",
    "response = requests.get(base_url + query)\n",
    "\n",
    "# parse the response using feedparser\n",
    "feed = feedparser.parse(response.text)\n",
    "\n",
    "# print out feed information\n",
    "print(\"Feed title: %s\" % feed.feed.title)\n",
    "print(\"Feed last updated: %s\" % feed.feed.updated)\n",
    "\n",
    "# print opensearch metadata\n",
    "print(\"totalResults for this query: %s\" % feed.feed.opensearch_totalresults)\n",
    "print(\"itemsPerPage for this query: %s\" % feed.feed.opensearch_itemsperpage)\n",
    "print(\"startIndex for this query: %s\" % feed.feed.opensearch_startindex)\n",
    "\n",
    "# Run through each entry, and print out information\n",
    "for entry in feed.entries:\n",
    "    print(\"e-print metadata\")\n",
    "    print(\"arxiv-id: %s\" % entry.id.split(\"/abs/\")[-1])\n",
    "    print(\"Published: %s\" % entry.published)\n",
    "    print(\"Title:  %s\" % entry.title)\n",
    "\n",
    "    # Authors\n",
    "    authors = entry.authors\n",
    "    if authors:\n",
    "        print(\"Authors:  %s\" % \",\".join(author.name for author in authors))\n",
    "\n",
    "    # get the links to the abs page and pdf for this e-print\n",
    "    for link in entry.links:\n",
    "        if link.rel == \"alternate\":\n",
    "            print(\"abs page link: %s\" % link.href)\n",
    "        elif link.title == \"pdf\":\n",
    "            print(\"pdf link: %s\" % link.href)\n",
    "\n",
    "    # The journal reference, comments and primary_category sections live under\n",
    "    # the arxiv namespace\n",
    "    journal_ref = getattr(entry, 'arxiv_journal_ref', 'No journal ref found')\n",
    "    print(\"Journal reference: %s\" % journal_ref)\n",
    "\n",
    "    comment = getattr(entry, 'arxiv_comment', 'No comment found')\n",
    "    print(\"Comments: %s\" % comment)\n",
    "\n",
    "    primary_category = entry.tags[0][\"term\"]\n",
    "    print(\"Primary Category: %s\" % primary_category)\n",
    "\n",
    "    all_categories = [t[\"term\"] for t in entry.tags]\n",
    "    print(\"All Categories: %s\" % (\", \").join(all_categories))\n",
    "\n",
    "    print(\"Abstract: %s\" % entry.summary)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7cc5032e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|#########################################################################################################| 2/2 [00:00<00:00,  3.36it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import AsyncHtmlLoader\n",
    "from langchain_community.document_transformers import BeautifulSoupTransformer\n",
    "import json\n",
    "\n",
    "\n",
    "# Load HTML\n",
    "links = [\"https://habr.com/ru/companies/sberbank/articles/805337/\",\n",
    "         \"https://habr.com/ru/feed/\"]\n",
    "\n",
    "loader = AsyncHtmlLoader(links)\n",
    "html = loader.load()\n",
    "\n",
    "bs_transformer = BeautifulSoupTransformer()\n",
    "docs_transformed = bs_transformer.transform_documents(html, tags_to_extract=[\"span\",\"p\"])\n",
    "\n",
    "result = {}\n",
    "for i in range(len(links)):\n",
    "    result[links[i]] = docs_transformed[i].page_content\n",
    "    \n",
    "f = open(\"assets/pages_content.json\",\"w\")\n",
    "f.write(json.dumps(result))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9440258",
   "metadata": {},
   "source": [
    "## Инициализация модели\n",
    "Теперь инициализируем модель GigaChat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff275711",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"credentials.txt\", 'r', encoding='utf-8') as file:\n",
    "    credentials = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7c95c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models.gigachat import GigaChat\n",
    "llm = GigaChat(credentials=credentials, scope=\"GIGACHAT_API_CORP\", verify_ssl_certs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5d10da",
   "metadata": {},
   "source": [
    "Для проверки спросим у модели вопрос про цвет плаща без какого-либо контекста. Возможно, она и так будет давать ожидаемый ответ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60651cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'YOLOv8 использует несколько видов потерь для обучения модели. Однако, основной вид потери, который используется в большинстве случаев, это Cross-Entropy Loss (CEL). \\n\\nCross-Entropy Loss (CEL) - это фу'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "\n",
    "question = \"Какой Loss использует Yolov8?\"\n",
    "llm([HumanMessage(content=question)]).content[0:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4606c691",
   "metadata": {},
   "source": [
    "Видим, что модель не отвечает так, как нам хотелось бы, поэтому применим RAG-подход.\n",
    "\n",
    "## Подготовка документа\n",
    "\n",
    "Для работы с документом нам нужно разделить его на части. Для этого используем `TextLoader` для загрузки книги и `RecursiveCharacterTextSplitter`, чтобы разделить текст на приблизительно равные куски в ~1000 символов с перекрытием в ~200 символов. Этот тип сплиттера сам выбирает каким способом следует оптимально разделять документ (по абзацам, по предложениям и т.д.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1795516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 46\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "loader = TextLoader(\"assets/yolov8_paper.txt\")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "documents = text_splitter.split_documents(documents)\n",
    "print(f\"Total documents: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc5c48d",
   "metadata": {},
   "source": [
    "После нарезки мы получили 91 документ частями книги.\n",
    "\n",
    "## Создание базы данных эмбеддингов\n",
    "\n",
    "Эмбеддинг это векторное представление текста, которое может быть использовано для определения смысловой близости текстов. Векторная база данных хранит тексты и соответствующие им эмбеддинги, а также умеет выполнять поиск по ним. Для работы с базой данных мы создаем объект GigaChatEmbeddings и передаем его в базу данных Chroma.\n",
    "\n",
    "> Обратите внимание, что сервис для вычисления эмбеддингов может тарифицироваться отдельно от стоимости модели GigaChat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f619ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.config import Settings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.embeddings.gigachat import GigaChatEmbeddings\n",
    "\n",
    "embeddings = GigaChatEmbeddings(\n",
    "    credentials=credentials, scope=\"GIGACHAT_API_CORP\", verify_ssl_certs=False\n",
    ")\n",
    "\n",
    "db = Chroma.from_documents(\n",
    "    documents,\n",
    "    embeddings,\n",
    "    client_settings=Settings(anonymized_telemetry=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b4f2bc",
   "metadata": {},
   "source": [
    "## Поиск по базе данных\n",
    "\n",
    "Теперь можно обратиться к базе данных и попросить найти документы, которые с наибольшей вероятностью содержат ответ на наш вопрос.\n",
    "\n",
    "По-умолчанию база данных возвращает 4 наиболее релевантных документа. Этот параметр можно изменить в зависимости от решаемой задачи и типа документов.\n",
    "\n",
    "Видно, что первый же документ содержит внутри себя часть книги с ответом на наш вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bba0150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = db.similarity_search(question, k=4)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6ecaa81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... tasets. We also witness YOLOv8 outperforming YOLOv5 for each RF100 category. From Figure 7b we\\ncan see that YOLOv8 produces similar or even better results\\ncompared to YOLOv5 [16] ...\n"
     ]
    }
   ],
   "source": [
    "print(f\"... {str(docs[0])[620:800]} ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d517ea",
   "metadata": {},
   "source": [
    "## QnA цепочка\n",
    "\n",
    "Теперь мы создадим цепочку QnA, которая специально предназначена для ответов на вопросы по документам. В качестве аргументов есть передается языковая модель и ретривер (база данных)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0eb76452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm, retriever=db.as_retriever())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b833cf",
   "metadata": {},
   "source": [
    "Наконец можно задать вопрос нашей цепочке и получить правильный ответ!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83ba8d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Какой Loss использует Yolov8?',\n",
       " 'result': 'YOLOv8 использует функцию потерь, которая состоит из двух частей: локальной потери и потери классификации. Локальная потеря измеряет ошибку между предсказанными областями и истинными областями, в то время как потеря классификации измеряет ошибку между предсказанными классами и истинными классами.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe17c484",
   "metadata": {},
   "source": [
    "Несколько дополнительных вопросов для проверки работоспособности:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d9d7907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Напиши скрипт для тестирования Yolov8',\n",
       " 'result': 'Извините, но я не могу написать скрипт для тестирования Yolov8, так как я не знаю, что это такое. Если вы можете предоставить больше информации или контекста, я смогу помочь вам лучше.'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain({\"query\": \"Напиши скрипт для тестирования Yolov8\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ee47c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Основные особенности Yolov8',\n",
       " 'result': 'YOLOv8 имеет несколько особенностей, которые отличают его от других версий YOLO. Во-первых, он использует новую архитектуру, которая сочетает в себе модули FAN и PAN. Это позволяет ему лучше захватывать особенности на разных масштабах и разрешениях, что важно для точного обнаружения объектов разного размера и формы. Кроме того, YOLOv8 превосходит YOLOv5 по нескольким параметрам, включая более высокую метрику mAP и меньшее количество выбросов при измерении против RF100. Он также превосходит YOLOv5 для каждого RF100 категории.'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain({\"query\": \"Основные особенности Yolov8\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894257e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "giga_kernel",
   "language": "python",
   "name": "giga_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
