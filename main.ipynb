{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ff0dd139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./2404.15626v1.An_Electromagnetism_Inspired_Method_for_Estimating_In_Grasp_Torque_from_Visuotactile_Sensors.pdf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "\n",
    "# Construct the default API client.\n",
    "client = arxiv.Client()\n",
    "\n",
    "search = arxiv.Search(\n",
    "    query = \"Attention is all you need\",\n",
    "    max_results = 10,\n",
    "    sort_by=arxiv.SortCriterion.SubmittedDate, \n",
    "    sort_order=arxiv.SortOrder.Descending\n",
    ")\n",
    "\n",
    "results = client.results(search)\n",
    "# `results` is a generator; you can iterate over its elements one by one...\n",
    "for r in client.results(search):\n",
    "    print(r.download_pdf())\n",
    "    print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb29c48",
   "metadata": {},
   "source": [
    "## Инициализация модели\n",
    "Теперь инициализируем модель GigaChat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "34027d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"credentials.txt\", 'r', encoding='utf-8') as file:\n",
    "    credentials = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8fc6c1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models.gigachat import GigaChat\n",
    "\n",
    "llm = GigaChat(credentials=credentials, scope=\"GIGACHAT_API_CORP\", verify_ssl_certs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "d431d468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 48\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"assets/arxiv_paper.pdf\")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "documents = text_splitter.split_documents(documents)\n",
    "print(f\"Total documents: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1306afce",
   "metadata": {},
   "source": [
    "После нарезки мы получили 91 документ частями книги.\n",
    "\n",
    "## Создание базы данных эмбеддингов\n",
    "\n",
    "Эмбеддинг это векторное представление текста, которое может быть использовано для определения смысловой близости текстов. Векторная база данных хранит тексты и соответствующие им эмбеддинги, а также умеет выполнять поиск по ним. Для работы с базой данных мы создаем объект GigaChatEmbeddings и передаем его в базу данных Chroma.\n",
    "\n",
    "> Обратите внимание, что сервис для вычисления эмбеддингов может тарифицироваться отдельно от стоимости модели GigaChat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "07022081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.config import Settings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.embeddings.gigachat import GigaChatEmbeddings\n",
    "\n",
    "embeddings = GigaChatEmbeddings(\n",
    "    credentials=credentials, scope=\"GIGACHAT_API_CORP\", verify_ssl_certs=False\n",
    ")\n",
    "\n",
    "db = Chroma.from_documents(\n",
    "    documents,\n",
    "    embeddings,\n",
    "    client_settings=Settings(anonymized_telemetry=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa604b2",
   "metadata": {},
   "source": [
    "## Поиск по базе данных\n",
    "\n",
    "Теперь можно обратиться к базе данных и попросить найти документы, которые с наибольшей вероятностью содержат ответ на наш вопрос.\n",
    "\n",
    "По-умолчанию база данных возвращает 4 наиболее релевантных документа. Этот параметр можно изменить в зависимости от решаемой задачи и типа документов.\n",
    "\n",
    "Видно, что первый же документ содержит внутри себя часть книги с ответом на наш вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "6af47b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = db.similarity_search(question, k=4)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "25d51d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... ение (fork) открытой библиотеки LangСhain на Python. Её главная цель — облегчить жизнь разработчику. Библиотека состоит из большого количества различных компонентов, которые позвол ...\n"
     ]
    }
   ],
   "source": [
    "print(f\"... {str(docs[0])[620:800]} ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fc42b5",
   "metadata": {},
   "source": [
    "## QnA цепочка\n",
    "\n",
    "Теперь мы создадим цепочку QnA, которая специально предназначена для ответов на вопросы по документам. В качестве аргументов есть передается языковая модель и ретривер (база данных)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "0cd2422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm, retriever=db.as_retriever(), return_source_documents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1f98b4",
   "metadata": {},
   "source": [
    "Наконец можно задать вопрос нашей цепочке и получить правильный ответ!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "c23c7fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Какой Loss использует Yolov8?',\n",
       " 'result': 'Я не знаю ответа на этот вопрос.',\n",
       " 'source_documents': [Document(page_content='Какой плащ был у Понтия Пилата? Отвечает GigaChat\\nСредний\\n10 мин\\n6.4K\\nБлог компании SberDevices\\nМашинное обучение\\n*\\nИскусственный интеллект\\nNatural Language Processing\\n*\\nОбзор\\n\\nВсем привет! На связи лид разработки SDK GigaChat’a — Константин Крестников. В этой статье я расскажу о том, что такое GigaChain и как в целом SDK позволяет упростить жизнь разработчика LLM, например, научить LLM давать ответы на вопросы по вашим документам или работать в режиме автономного агента. Также поговорим про решения, которые практически невозможно сделать без использования SDK.\\n\\nGigaСhain — это ответвление (fork) открытой библиотеки LangСhain на Python. Её главная цель — облегчить жизнь разработчику. Библиотека состоит из большого количества различных компонентов, которые позволяют работать с промптами, объединять вызовы к большим языковым моделям в цепочки, загружать данные из разных источников и сохранять обработанные ответы языковой модели.', metadata={'source': 'assets/habr_paper.txt'}),\n",
       "  Document(page_content='SDK GigaChain отличается от LangChain тем, что он переведен на русский язык, адаптирован к экосистеме русскоязычных языковых моделей и, конечно же, к GigaChat. В нем есть большое количество различных утилит: \\n\\nБиблиотека на Python содержит интерфейсы и интеграции для множества компонентов, базовую среду выполнения для объединения этих компонентов в цепочки и агенты, а также примеры их реализаций.\\n\\nХаб промптов — набор типовых отлаженных промптов для решения различных задач.\\n\\nGigaServe — библиотека для публикации цепочек GigaChain как REST API.\\n\\nКроме этого, фреймворк совместим со сторонним сервисом LangSmith — платформой для разработчиков, которая позволяет отлаживать, тестировать, оценивать и отслеживать цепочки, построенные на любой платформе LLM, и легко интегрируется с LangChain и GigaChain.', metadata={'source': 'assets/habr_paper.txt'}),\n",
       "  Document(page_content='Почему мы приняли решение делать свой форк, а не добавлять поддержку Gigachat в LangChain? Всё дело в том, что в проекте, который построен вокруг NLP, содержится огромное количество английского текста, который ровным слоем распределен по всей библиотеке, а PR с неплохим решением для мультиязычности авторы LangChain не принимают. Штош.\\n\\nКак начать использовать GigaChain\\nGigaChain как открытая библиотека опубликована в различных репозиториях, в том числе и в pypi, поэтому вам достаточно написать:\\n\\npip install gigachain\\nИ SDK появится в вашем окружении. Дальше мы импортируем объект gigachat и начинаем им пользоваться.\\n\\nДля авторизации запросов к GigaChat вам нужно получить авторизационные данные для работы с GigaChat API.\\n\\nПередайте полученные авторизационные данные в параметре credentials объекта GigaChat. Также потребуется установить сертификаты МинЦифры или же отключить проверку ssl с помощью флага verify_ssl_certs=False. Подробнее про настройку авторизации.', metadata={'source': 'assets/habr_paper.txt'}),\n",
       "  Document(page_content='Элементарная реализация чат-бота, с которым можно пообщаться.\\nЭлементарная реализация чат-бота, с которым можно пообщаться.\\nUser: Привет\\nBot:  Здравствуйте!\\nUser: Ты кто?\\nBot:  Я -- виртуальный помощник.\\nUser: Как тебя зовут?\\nBot:  Меня зовут GigaChat.\\nБиблиотека GigaChain обратно совместима с LangChain, что позволяет использовать ее не только для работы с GigaChat, но и для работы с другими LLM в различных комбинациях.\\n\\nПодробная документация для GigaChain доступна в репозитории.\\n\\nЧто можно сделать с помощью GigaChain\\nGigaChain содержит большое количество инструментов и примеров, которые можно использовать в качестве основы своего проекта или просто изучать для расширения кругозора в области возможных кейсов применений LLM.\\n\\nПри решении задач с помощью языковых моделей, часто необходимо учитывать различные дополнительные факты и данные при формировании ответа, например:', metadata={'source': 'assets/habr_paper.txt'})]}"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78ea0b2",
   "metadata": {},
   "source": [
    "Несколько дополнительных вопросов для проверки работоспособности:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "43e8d592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Расскажи про тимлида Сбера',\n",
       " 'result': 'Извините, но у меня нет информации о конкретном тимлиде Сбера. Могу рассказать вам о том, что такое тимлид и какие у него обязанности. Тимлид - это лидер команды, который отвечает за управление командой, ее развитие и достижение поставленных целей. Он координирует работу членов команды, помогает им решать возникающие проблемы, поддерживает мотивацию и вовлеченность. Кроме того, тимлид также участвует в принятии решений, связанных с проектами, и помогает команде в достижении общих целей.'}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain({\"query\": \"Расскажи про тимлида Сбера\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "423e61eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Основные особенности GigaChain',\n",
       " 'result': 'GigaChain обладает следующими особенностями:\\n\\n1. Обратная совместимость с LangChain: GigaChain может использоваться вместе с другими LLM в различных комбинациях.\\n\\n2. Большое количество инструментов и примеров: GigaChain содержит большое количество инструментов и примеров, которые можно использовать в качестве основы своего проекта или просто изучать для расширения кругозора в области возможных кейсов применения LLM.\\n\\n3. Реализация агентов: GigaChain позволяет создавать агентов, которые могут взаимодействовать с внешней средой и корректировать свое поведение в зависимости от результатов взаимодействия.\\n\\n4. Загрузка документов: GigaChain позволяет загружать документы пользователя в память и разрезать их на части для дальнейшего анализа.\\n\\n5. Работа с эмбеддингами: Каждая часть документа преобразуется в эмбеддинг - числовой вектор, который представляет собой внутреннее представление текста моделью.\\n\\n6. Работа с базами данных: GigaChain поддерживает работу с различными базами данных, такими как Chroma и FAISS.',\n",
       " 'source_documents': [Document(page_content='Элементарная реализация чат-бота, с которым можно пообщаться.\\nЭлементарная реализация чат-бота, с которым можно пообщаться.\\nUser: Привет\\nBot:  Здравствуйте!\\nUser: Ты кто?\\nBot:  Я -- виртуальный помощник.\\nUser: Как тебя зовут?\\nBot:  Меня зовут GigaChat.\\nБиблиотека GigaChain обратно совместима с LangChain, что позволяет использовать ее не только для работы с GigaChat, но и для работы с другими LLM в различных комбинациях.\\n\\nПодробная документация для GigaChain доступна в репозитории.\\n\\nЧто можно сделать с помощью GigaChain\\nGigaChain содержит большое количество инструментов и примеров, которые можно использовать в качестве основы своего проекта или просто изучать для расширения кругозора в области возможных кейсов применений LLM.\\n\\nПри решении задач с помощью языковых моделей, часто необходимо учитывать различные дополнительные факты и данные при формировании ответа, например:', metadata={'source': 'assets/habr_paper.txt'}),\n",
       "  Document(page_content='или так:\\n\\nСистемы, способные на широкий спектр действий и достаточно надежны, чтобы в определенных обстоятельствах пользователь мог доверить им эффективно и автономно выполнять задачи для достижения сложных целей ВМЕСТО пользователя.\\n\\nА Билл Гейтс говорит, что агенты — это тип программного обеспечения, который реагирует на естественный язык и может выполнять множество различных задач на основе знаний пользователя. \\n\\nЯ бы дополнил определение агентов следующим тезисом:\\n\\n«Агент – это программа, которая способна взаимодействовать с внешней средой с помощью инструментов и корректировать своё поведение в зависимости от результатов взаимодействия (рефлексия).\\n\\nБез рефлексии агент превращается в цепочку вызовов модели».\\n\\nИ да, GigaChain со своими возможностями является ничем иным, как инструментом для создания агентов.', metadata={'source': 'assets/habr_paper.txt'}),\n",
       "  Document(page_content='Для начала, с помощью различных адаптеров, мы загружаем документы пользователя в память. В рамках GigaChain таких адаптеров-загрузчиков достаточное количество. Вы можете загружать PDF, текстовые файлы, можно загружать данные из интернета, из википедии, из каких-то других источников, например, патентной базы данных.\\n\\nПосле этого каждый документ «разрезается» на некоторое количество частей и для каждой части считается эмбеддинг. \\n\\nЭмбеддинг — это числовой вектор, внутреннее представление текста моделью. Было замечено, что у текстов со сходными смыслами — сходные эмбеддинги. Затем такие векторы смыслов складываются в специальную векторную базу данных. В рамках GigaChain поддерживаются самые популярные базы, например, Chroma и FAISS.', metadata={'source': 'assets/habr_paper.txt'}),\n",
       "  Document(page_content='Какой плащ был у Понтия Пилата? Отвечает GigaChat\\nСредний\\n10 мин\\n6.4K\\nБлог компании SberDevices\\nМашинное обучение\\n*\\nИскусственный интеллект\\nNatural Language Processing\\n*\\nОбзор\\n\\nВсем привет! На связи лид разработки SDK GigaChat’a — Константин Крестников. В этой статье я расскажу о том, что такое GigaChain и как в целом SDK позволяет упростить жизнь разработчика LLM, например, научить LLM давать ответы на вопросы по вашим документам или работать в режиме автономного агента. Также поговорим про решения, которые практически невозможно сделать без использования SDK.\\n\\nGigaСhain — это ответвление (fork) открытой библиотеки LangСhain на Python. Её главная цель — облегчить жизнь разработчику. Библиотека состоит из большого количества различных компонентов, которые позволяют работать с промптами, объединять вызовы к большим языковым моделям в цепочки, загружать данные из разных источников и сохранять обработанные ответы языковой модели.', metadata={'source': 'assets/habr_paper.txt'})]}"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain({\"query\": \"Основные особенности GigaChain\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d81f740c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import os\n",
    "import json\n",
    "\n",
    "def download_from_arxiv(key_word, max_results=10, destination_path=\"assets/arxiv/\", saved_list_path=\"assets/arxiv/papers.json\"):\n",
    "    client = arxiv.Client()\n",
    "\n",
    "    search = arxiv.Search(\n",
    "        query = str(key_word),\n",
    "        max_results = max_results,\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate, \n",
    "        sort_order=arxiv.SortOrder.Descending\n",
    "    )\n",
    "    # Проверка существования файла с сохраненным списком скачанных arXiv ID\n",
    "    if os.path.exists(saved_list_path):\n",
    "        with open(saved_list_path, \"r\") as f:\n",
    "            downloaded_arxiv_ids = json.load(f)\n",
    "    else:\n",
    "        downloaded_arxiv_ids = []\n",
    "    \n",
    "    for result in client.results(search):\n",
    "        # Проверка наличия статьи в списке уже скачанных\n",
    "        if result.entry_id.split(\"/\")[-1] in downloaded_arxiv_ids:\n",
    "            print(f\"Статья {result.entry_id} уже скачана и пропущена.\")\n",
    "            continue\n",
    "        \n",
    "        # Скачивание PDF-файла статьи\n",
    "        pdf_path = result.download_pdf(dirpath=destination_path)\n",
    "        if pdf_path:\n",
    "            print(f\"Статья {result.entry_id} успешно скачана и сохранена в {pdf_path}\")\n",
    "            # Добавление arXiv ID в список скачанных\n",
    "            downloaded_arxiv_ids.append(result.entry_id.split(\"/\")[-1])\n",
    "    # Сохранение списка скачанных arXiv ID в файл JSON\n",
    "    with open(saved_list_path, \"w\") as f:\n",
    "        json.dump(downloaded_arxiv_ids, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1637c444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статья http://arxiv.org/abs/2404.15238v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15104v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.14977v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.14963v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.14943v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15382v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.14809v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.14740v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.14695v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.14631v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15488v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.14043v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.13948v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.13892v2 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.13781v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.12879v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.12772v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.12560v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.12457v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.12309v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.14901v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.13957v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.13764v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.13633v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.12926v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.12309v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.09577v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15351v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.09375v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.13066v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15604v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15592v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15588v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15578v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15549v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15522v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15515v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15488v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15485v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15458v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15615v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15608v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15604v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15603v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15592v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15591v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15580v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15576v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15564v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15552v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.06751v1 успешно скачана и сохранена в assets/arxiv/2404.06751v1.Leveraging_open_source_models_for_legal_language_modeling_and_analysis__a_case_study_on_the_Indian_constitution.pdf\n",
      "Статья http://arxiv.org/abs/2403.15729v2 успешно скачана и сохранена в assets/arxiv/2403.15729v2.Towards_a_RAG_based_Summarization_Agent_for_the_Electron_Ion_Collider.pdf\n",
      "Статья http://arxiv.org/abs/2403.14258v1 успешно скачана и сохранена в assets/arxiv/2403.14258v1.LLM_based_Extraction_of_Contradictions_from_Patents.pdf\n",
      "Статья http://arxiv.org/abs/2403.14702v1 успешно скачана и сохранена в assets/arxiv/2403.14702v1.Large_language_model_powered_chatbots_for_internationalizing_student_support_in_higher_education.pdf\n",
      "Статья http://arxiv.org/abs/2403.10482v2 успешно скачана и сохранена в assets/arxiv/2403.10482v2.Can_a_GPT4_Powered_AI_Agent_Be_a_Good_Enough_Performance_Attribution_Analyst_.pdf\n",
      "Статья http://arxiv.org/abs/2403.10588v1 успешно скачана и сохранена в assets/arxiv/2403.10588v1.S3LLM__Large_Scale_Scientific_Software_Understanding_with_LLMs_using_Source__Metadata__and_Document.pdf\n",
      "Статья http://arxiv.org/abs/2403.00830v1 успешно скачана и сохранена в assets/arxiv/2403.00830v1.MedAide__Leveraging_Large_Language_Models_for_On_Premise_Medical_Assistance_on_Edge_Devices.pdf\n",
      "Статья http://arxiv.org/abs/2403.05568v1 успешно скачана и сохранена в assets/arxiv/2403.05568v1.Revolutionizing_Mental_Health_Care_through_LangChain__A_Journey_with_a_Large_Language_Model.pdf\n",
      "Статья http://arxiv.org/abs/2402.06929v1 успешно скачана и сохранена в assets/arxiv/2402.06929v1.Making_a_prototype_of_Seoul_historical_sites_chatbot_using_Langchain.pdf\n",
      "Статья http://arxiv.org/abs/2402.01733v1 успешно скачана и сохранена в assets/arxiv/2402.01733v1.Development_and_Testing_of_Retrieval_Augmented_Generation_in_Large_Language_Models____A_Case_Study_Report.pdf\n"
     ]
    }
   ],
   "source": [
    "key_words = [\"NLP\", \"RAG\", \"ChatBot\", \"LLM\", \"Speech Recognition\", \"LangChain\", \"LLM Agents\"]\n",
    "\n",
    "for key_word in key_words:\n",
    "    # Пример использования функции\n",
    "    download_from_arxiv(key_word, max_results=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "45fb8bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_files(directory):\n",
    "    pdf_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".pdf\"):\n",
    "                pdf_files.append(os.path.join(root, file))\n",
    "    return pdf_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6a521241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assets/arxiv/2404.15604v1.Hybrid_LLM_Rule_based_Approaches_to_Business_Insights_Generation_from_Structured_Data.pdf\n",
      "assets/arxiv/2404.15588v1.Minimal_Evidence_Group_Identification_for_Claim_Verification.pdf\n",
      "assets/arxiv/2404.15578v1.Can_Foundational_Large_Language_Models_Assist_with_Conducting_Pharmaceuticals_Manufacturing_Investigations_.pdf\n",
      "assets/arxiv/2404.15564v1.Guided_AbsoluteGrad__Magnitude_of_Gradients_Matters_to_Explanation_s_Localization_and_Saliency.pdf\n",
      "assets/arxiv/2404.15592v1.ImplicitAVE__An_Open_Source_Dataset_and_Multimodal_LLMs_Benchmark_for_Implicit_Attribute_Value_Extraction.pdf\n",
      "assets/arxiv/2404.13892v2.Retrieval_Augmented_Audio_Deepfake_Detection.pdf\n",
      "assets/arxiv/2404.14631v1.Learning_Word_Embedding_with_Better_Distance_Weighting_and_Window_Size_Scheduling.pdf\n",
      "assets/arxiv/2404.15552v1.Cross_Temporal_Spectrogram_Autoencoder__CTSAE___Unsupervised_Dimensionality_Reduction_for_Clustering_Gravitational_Wave_Glitches.pdf\n",
      "assets/arxiv/2404.14901v1.Beyond_Code_Generation__An_Observational_Study_of_ChatGPT_Usage_in_Software_Engineering_Practice.pdf\n",
      "assets/arxiv/2404.15615v1.MDDD__Manifold_based_Domain_Adaptation_with_Dynamic_Distribution_for_Non_Deep_Transfer_Learning_in_Cross_subject_and_Cross_session_EEG_based_Emotion_Recognition.pdf\n",
      "assets/arxiv/2404.13781v1.Evaluating_Retrieval_Quality_in_Retrieval_Augmented_Generation.pdf\n",
      "assets/arxiv/2404.09375v1.Deceptive_Patterns_of_Intelligent_and_Interactive_Writing_Assistants.pdf\n",
      "assets/arxiv/2404.12560v1.Dubo_SQL__Diverse_Retrieval_Augmented_Generation_and_Fine_Tuning_for_Text_to_SQL.pdf\n",
      "assets/arxiv/2404.15382v1.Feature_Distribution_Shift_Mitigation_with_Contrastive_Pretraining_for_Intrusion_Detection.pdf\n",
      "assets/arxiv/2404.14740v1.Modeling_the_Sacred__Considerations_when_Using_Considerations_when_Using_Religious_Texts_in_Natural_Language_Processing.pdf\n",
      "assets/arxiv/2404.14943v1.Does_It_Make_Sense_to_Explain_a_Black_Box_With_Another_Black_Box_.pdf\n",
      "assets/arxiv/2404.12457v1.RAGCache__Efficient_Knowledge_Caching_for_Retrieval_Augmented_Generation.pdf\n",
      "assets/arxiv/2404.15608v1.Understanding_and_Improving_CNNs_with_Complex_Structure_Tensor__A_Biometrics_Study.pdf\n",
      "assets/arxiv/2404.09577v1.Transformers__Contextualism__and_Polysemy.pdf\n",
      "assets/arxiv/2404.15591v1.Domain_Adaptation_for_Learned_Image_Compression_with_Supervised_Adapters.pdf\n",
      "assets/arxiv/2404.15522v1.Towards_Systematic_Evaluation_of_Logical_Reasoning_Ability_of_Large_Language_Models.pdf\n",
      "assets/arxiv/2404.13066v1.Leveraging_Large_Language_Model_as_Simulated_Patients_for_Clinical_Education.pdf\n",
      "assets/arxiv/2404.13764v1.Using_Adaptive_Empathetic_Responses_for_Teaching_English.pdf\n",
      "assets/arxiv/2404.15603v1.Development_of_Pattern_Recognition_Validation_for_Boson_Sampling.pdf\n",
      "assets/arxiv/2404.14695v1.MisgenderMender__A_Community_Informed_Approach_to_Interventions_for_Misgendering.pdf\n",
      "assets/arxiv/2404.15458v1.Can_Large_Language_Models_Learn_the_Physics_of_Metamaterials__An_Empirical_Study_with_ChatGPT.pdf\n",
      "assets/arxiv/2404.14043v1.LLMs_Know_What_They_Need__Leveraging_a_Missing_Information_Guided_Framework_to_Empower_Retrieval_Augmented_Generation.pdf\n",
      "assets/arxiv/2404.15238v1.CultureBank__An_Online_Community_Driven_Knowledge_Base_Towards_Culturally_Aware_Language_Technologies.pdf\n",
      "assets/arxiv/2404.15580v1.MiM__Mask_in_Mask_Self_Supervised_Pre_Training_for_3D_Medical_Image_Analysis.pdf\n",
      "assets/arxiv/2404.15576v1.Designing_AI_Enabled_Games_to_Support_Social_Emotional_Learning_for_Children_with_Autism_Spectrum_Disorders.pdf\n",
      "assets/arxiv/2404.12926v1.MM_PhyRLHF__Reinforcement_Learning_Framework_for_Multimodal_Physics_Question_Answering.pdf\n",
      "assets/arxiv/2404.12879v1.Unlocking_Multi_View_Insights_in_Knowledge_Dense_Retrieval_Augmented_Generation.pdf\n",
      "assets/arxiv/2404.15485v1.Large_Language_Models_Spot_Phishing_Emails_with_Surprising_Accuracy__A_Comparative_Analysis_of_Performance.pdf\n",
      "assets/arxiv/2404.15549v1.PRISM__Patient_Records_Interpretation_for_Semantic_Clinical_Trial_Matching_using_Large_Language_Models.pdf\n",
      "assets/arxiv/2404.12772v1.Generating_Test_Scenarios_from_NL_Requirements_using_Retrieval_Augmented_LLMs__An_Industrial_Study.pdf\n",
      "assets/arxiv/2404.14977v1.Social_Media_and_Artificial_Intelligence_for_Sustainable_Cities_and_Societies__A_Water_Quality_Analysis_Use_case.pdf\n",
      "assets/arxiv/2404.14963v1.Achieving__97__on_GSM8K__Deeply_Understanding_the_Problems_Makes_LLMs_Perfect_Reasoners.pdf\n",
      "assets/arxiv/2404.15351v1.Integrating_Physiological_Data_with_Large_Language_Models_for_Empathic_Human_AI_Interaction.pdf\n",
      "assets/arxiv/2404.13957v1.How_Well_Can_LLMs_Echo_Us__Evaluating_AI_Chatbots__Role_Play_Ability_with_ECHO.pdf\n",
      "assets/arxiv/2404.15104v1.Identifying_Fairness_Issues_in_Automatically_Generated_Testing_Content.pdf\n",
      "assets/arxiv/2404.12309v1.iRAG__An_Incremental_Retrieval_Augmented_Generation_System_for_Videos.pdf\n",
      "assets/arxiv/2404.14809v1.A_Survey_of_Large_Language_Models_on_Generative_Graph_Analytics__Query__Learning__and_Applications.pdf\n",
      "assets/arxiv/2404.13948v1.Typos_that_Broke_the_RAG_s_Back__Genetic_Attack_on_RAG_Pipeline_by_Simulating_Documents_in_the_Wild_via_Low_level_Perturbations.pdf\n",
      "assets/arxiv/2404.15515v1.ToM_LM__Delegating_Theory_Of_Mind_Reasoning_to_External_Symbolic_Executors_in_Large_Language_Models.pdf\n",
      "assets/arxiv/2404.15488v1.IryoNLP_at_MEDIQA_CORR_2024__Tackling_the_Medical_Error_Detection___Correction_Task_On_the_Shoulders_of_Medical_Agents.pdf\n",
      "assets/arxiv/2404.13633v1.Incorporating_Different_Verbal_Cues_to_Improve_Text_Based_Computer_Delivered_Health_Messaging.pdf\n"
     ]
    }
   ],
   "source": [
    "# Пример использования функции\n",
    "directory = \"assets/arxiv/\"\n",
    "pdf_files = get_pdf_files(directory)\n",
    "for pdf_file in pdf_files:\n",
    "    print(pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "05031d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "\n",
    "def summarize(path_to_pdf_file, chunk_size=5000, chunk_overlap=1000): \n",
    "    loader = PyMuPDFLoader(path_to_pdf_file)\n",
    "    docs = loader.load()\n",
    "    split_docs = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap).split_documents(\n",
    "        docs\n",
    "    )\n",
    "    \n",
    "    giga = GigaChat(credentials=credentials, scope=\"GIGACHAT_API_CORP\", verify_ssl_certs=False)\n",
    "    chain = load_summarize_chain(giga, chain_type=\"map_reduce\")\n",
    "    res = chain.run(split_docs)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ef35ead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "class ArticlesDB:\n",
    "    def __init__(self, db_path='articles.db'):\n",
    "        self.conn = sqlite3.connect(db_path)\n",
    "        self.cursor = self.conn.cursor()\n",
    "        self.create_table()\n",
    "\n",
    "    def create_table(self):\n",
    "        self.cursor.execute('''CREATE TABLE IF NOT EXISTS articles\n",
    "                            (id INTEGER PRIMARY KEY,\n",
    "                            title TEXT NOT NULL,\n",
    "                            summary TEXT)''')\n",
    "        self.conn.commit()\n",
    "\n",
    "    def add_article(self, title, summary):\n",
    "        self.cursor.execute('INSERT INTO articles (title, summary) VALUES (?, ?)', (title, summary))\n",
    "        self.conn.commit()\n",
    "\n",
    "    def delete_article(self, title):\n",
    "        self.cursor.execute('DELETE FROM articles WHERE title = ?', (title,))\n",
    "        self.conn.commit()\n",
    "\n",
    "    def get_article_summary(self, title):\n",
    "        self.cursor.execute('SELECT summary FROM articles WHERE title = ?', (title,))\n",
    "        result = self.cursor.fetchone()\n",
    "        if result:\n",
    "            return result[0]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def get_all_articles(self):\n",
    "        self.cursor.execute('SELECT title, summary FROM articles')\n",
    "        return self.cursor.fetchall()\n",
    "\n",
    "    def close(self):\n",
    "        self.conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0c4c28a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_db = ArticlesDB(db_path=\"articles.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ae19db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for filename in tqdm(pdf_files, desc=\"Processing PDF files\"):\n",
    "    article_title = filename.split(\"/\")[-1][:-4]\n",
    "    article_summary = articles_db.get_article_summary(article_title)\n",
    "    if article_summary is None:\n",
    "        result = summarize(filename,\n",
    "                       chunk_size=5000,\n",
    "                       chunk_overlap=1000)\n",
    "        articles_db.add_article(article_title, result)\n",
    "        print(f\"Article \\\"{article_title}\\\" saved to database\")\n",
    "print(articles_db.get_all_articles())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "2055310f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2404.15604v1.Hybrid_LLM_Rule_based_Approaches_to_Business_Insights_Generation_from_Structured_Data\n",
      "The paper discusses the integration of rule-based systems and Large Language Models (LLMs) to generate actionable business insights from structured data. The hybrid approach combines the robustness of rule-based systems with the adaptive power of LLMs, addressing the complexities of data extraction in the field of business intelligence. The hybrid approach combines interpretable AI techniques, such as LIME, rule-based systems, and supervised document classification, to extract actionable insights from business data. LLM plays a vital role in this process by enhancing the understanding and generation of natural language-generated insights. The hybrid approach can uncover personalized and nuanced user interests, needs, and goals from user journeys and user activities on the platform. However, when implementing this approach, it is essential to consider data quality, domain knowledge, and scalability. Additionally, there are ethical concerns related to the use of large language models, as they can perpetuate biases and misinformation present in the training data.\n",
      "\n",
      "2404.15588v1.Minimal_Evidence_Group_Identification_for_Claim_Verification\n",
      "This paper presents a method for identifying minimal evidence groups (MEGs) in claim verification. The proposed approach significantly outperforms baseline methods on precision and F0.5 scores. The non-redundancy of MEGs improves the performance of downstream applications.\n",
      "\n",
      "2404.15578v1.Can_Foundational_Large_Language_Models_Assist_with_Conducting_Pharmaceuticals_Manufacturing_Investigations_\n",
      "This paper explores the potential of large language models (LLMs) in assisting with pharmaceutical manufacturing investigations. LLMs can extract specific information from unstructured data and identify similar or related deviations through semantic search on historical records. GPT-4 and Claude-2 show high accuracy in information extraction and similarity identification. However, the complex interplay between reasoning and hallucination behavior is a risk factor. Incidents in a manufacturing environment can lead to production halts, safety or environmental issues, and product quality concerns. Investigations aim to identify main causes, potential impacts, and preventive actions. Organizations store knowledge from previous incidents in two forms: tactic knowledge and formal records. Automated approaches enabled by IT capabilities, databases, and AI and data analytics tools can address incident records and lessons learned. The potential use of Natural Language Processing (NLP) techniques for automation, particularly with the introduction of Large Language Models (LLMs), is discussed. However, the challenges and barriers associated with the application of LLMs in industries with high accuracy standards, such as manufacturing, are also highlighted.\n",
      "\n",
      "2404.15564v1.Guided_AbsoluteGrad__Magnitude_of_Gradients_Matters_to_Explanation_s_Localization_and_Saliency\n",
      "This paper presents a new gradient-based XAI method called Guided AbsoluteGrad for saliency map explanations. The method utilizes both positive and negative gradient magnitudes and employs gradient variance to distinguish the important areas for noise deduction. It also introduces a novel evaluation metric named ReCover And Predict (RCAP), which considers the Localization and Visual Noise Level objectives of the explanations. The method is evaluated with seven gradient-based XAI methods using the RCAP metric and other SOTA metrics in three case studies: ImageNet dataset with ResNet50 model, International Skin Imaging Collaboration (ISIC) dataset with EfficientNet model, and Places365 dataset with DenseNet161 model. The method surpasses other gradient-based approaches, showcasing the quality of enhanced saliency map explanations through gradient magnitude.\n",
      "\n",
      "2404.15592v1.ImplicitAVE__An_Open_Source_Dataset_and_Multimodal_LLMs_Benchmark_for_Implicit_Attribute_Value_Extraction\n",
      "This paper presents ImplicitAVE, a publicly available multimodal dataset and benchmark for implicit attribute value extraction. The dataset addresses limitations of existing AVE datasets, such as focusing on explicit attribute values, lacking product images, and being limited to a few domains. ImplicitAVE includes 68,000 training and 1,600 testing data points across five domains, and it explores the application of multimodal large language models (MLLMs) for implicit AVE. The authors evaluate six recent MLLMs with eleven variants, revealing that implicit value extraction remains a challenging task for MLLMs.\n",
      "\n",
      "2404.13892v2.Retrieval_Augmented_Audio_Deepfake_Detection\n",
      "The paper presents a retrieval-augmented detection (RAD) framework for audio deepfake detection. The framework uses retrieval-augmented generation (RAG) to enhance detection by augmenting test samples with similar retrieved samples. The multi-fusion attentive classifier is also extended to integrate it with the proposed RAD framework. Experimental results show the superior performance of the proposed RAD framework over baseline methods, achieving state-of-the-art results on the ASVspoof 2021 DF set and competitive results on the 2019 and 2021 LA sets.\n",
      "\n",
      "2404.14631v1.Learning_Word_Embedding_with_Better_Distance_Weighting_and_Window_Size_Scheduling\n",
      "This paper proposes two novel methods, Learnable Formulated Weights (LFW) and Epoch-based Dynamic Window Size (EDWS), to incorporate distance information into two variants of Word2Vec, the Continuous Bag-of-Words (CBOW) model and the Continuous Skip-gram (Skip-gram) model. LFW uses a formula with learnable parameters to calculate distance-related weights for average pooling in CBOW, while EDWS improves the dynamic window size strategy in Skip-gram to introduce distance information in a more balanced way. Experiments show the effectiveness of these methods in enhancing Word2Vec's performance, surpassing previous state-of-the-art methods.\n",
      "\n",
      "2404.15552v1.Cross_Temporal_Spectrogram_Autoencoder__CTSAE___Unsupervised_Dimensionality_Reduction_for_Clustering_Gravitational_Wave_Glitches\n",
      "The Cross-Temporal Spectrogram Autoencoder (CTSAE) is an unsupervised method for dimensionality reduction and clustering of gravitational wave glitches. It integrates a four-branch autoencoder with convolutional neural networks and vision transformers. CTSAE was trained and evaluated on the GravitySpy O3 dataset, and demonstrated superior performance in clustering tasks compared to state-of-the-art semi-supervised learning methods. This pioneering unsupervised approach marks a significant step forward in the field of gravitational wave research.\n",
      "\n",
      "2404.14901v1.Beyond_Code_Generation__An_Observational_Study_of_ChatGPT_Usage_in_Software_Engineering_Practice\n",
      "This paper presents an observational study of professional software engineers' experiences with ChatGPT, a large language model, in their work. The study examines how practitioners use ChatGPT and the factors that influence their experience with the tool. The authors find that practitioners often use ChatGPT for guidance and learning rather than generating ready-to-use software artifacts. They also propose a theoretical framework for understanding the interaction between users, internal factors, and external factors in shaping the experience with ChatGPT.\n",
      "\n",
      "2404.15615v1.MDDD__Manifold_based_Domain_Adaptation_with_Dynamic_Distribution_for_Non_Deep_Transfer_Learning_in_Cross_subject_and_Cross_session_EEG_based_Emotion_Recognition\n",
      "The paper presents a novel non-deep transfer learning method called Manifold-based Domain Adaptation with Dynamic Distribution (MDDD) for cross-subject and cross-session EEG-based emotion recognition. The proposed MDDD model includes four main modules: manifold feature transformation, dynamic distribution alignment, classifier learning, and ensemble learning. The data is transformed onto an optimal Grassmann manifold space, allowing for dynamic alignment of the source and target domains. The classifier learning integrates the principle of structural risk minimization to develop robust classification models, while the ensemble learning module aggregates the classifiers obtained at different stages of the optimization process to enhance prediction accuracy. The MDDD model is validated with two well-known databases and two different validation methods (cross-subject single-session and cross-subject cross-session), demonstrating improved performance compared to traditional non-deep learning methods and comparable performance to deep learning methods.\n",
      "\n",
      "2404.13781v1.Evaluating_Retrieval_Quality_in_Retrieval_Augmented_Generation\n",
      "This paper presents eRAG, a novel evaluation approach for retrieval-augmented generation (RAG) systems. Instead of using traditional end-to-end evaluation methods, eRAG utilizes the large language model (LLM) within the RAG system to generate output for each document in the retrieval list. The generated output is then evaluated based on the downstream task ground truth labels. The downstream performance for each document serves as its relevance label. Various downstream task metrics are employed to obtain document-level annotations and aggregated using set-based or ranking metrics. Experiments on a wide range of datasets demonstrate that eRAG achieves a higher correlation with downstream RAG performance compared to baseline methods, with improvements in Kendall's τ correlation ranging from 0.168 to 0.494. Additionally, eRAG offers significant computational advantages, improving runtime and consuming up to 50 times less GPU memory than end-to-end evaluation.\n",
      "\n",
      "2404.09375v1.Deceptive_Patterns_of_Intelligent_and_Interactive_Writing_Assistants\n",
      "This paper discusses the potential use of deceptive user interface/user experience (UI/UX) patterns in interactive and intelligent writing assistants like ChatGPT and systems that help with text-related tasks. The authors aim to raise awareness about these patterns and encourage further research into how the design and interaction of such systems can affect users and their writing. The paper presents a set of deceptive UI/UX patterns for writing assistants, including manipulating the user interface to privilege certain actions, forcing users to perform certain actions, and hiding costs and charges. The potential motivation for these patterns is opinion influence and financial gains. The paper highlights the need for longitudinal user studies to understand the potential consequences of these patterns on user dependency and skill development.\n",
      "\n",
      "2404.12560v1.Dubo_SQL__Diverse_Retrieval_Augmented_Generation_and_Fine_Tuning_for_Text_to_SQL\n",
      "Dubo-SQL is a text-to-SQL method that improves execution accuracy on the BIRD-SQL benchmark. Dubo-SQL v1 uses GPT-3.5 Turbo and exceeds the performance of the next-best model using OpenAI. Dubo-SQL v2 uses GPT-4 Turbo and RAG to push EX higher. Dubo-SQL v1 costs $273 at OpenAI's pricing, while Dubo-SQL v2 costs under $0.14 per question.\n",
      "\n",
      "2404.15382v1.Feature_Distribution_Shift_Mitigation_with_Contrastive_Pretraining_for_Intrusion_Detection\n",
      "This paper presents SwapCon, a machine learning model designed to address the feature distribution shift problem in network intrusion detection. The model is pretrained using contrastive learning and swapping augmentation strategies to learn general patterns from a large dataset. The authors demonstrate that pretraining can increase robustness against feature distribution shift by over 8% and show that the proposed SwapCon model outperforms other models like eXtreme Gradient Boosting (XGBoost) and K-Nearest Neighbor (KNN) by a large margin.\n",
      "\n",
      "2404.14740v1.Modeling_the_Sacred__Considerations_when_Using_Considerations_when_Using_Religious_Texts_in_Natural_Language_Processing\n",
      "This paper discusses the use of religious texts in natural language processing (NLP) and its ethical implications. It argues that NLP's use of such texts raises considerations beyond model biases, including data provenance, cultural contexts, and their use in proselytism. The paper calls for more consideration of researcher positionality and the perspectives of marginalized linguistic and religious communities.\n",
      "\n",
      "2404.14943v1.Does_It_Make_Sense_to_Explain_a_Black_Box_With_Another_Black_Box_\n",
      "The paper compares transparent and opaque counterfactual explanation methods in NLP tasks. Transparent methods perturb the target document by adding, removing, or replacing words, while opaque approaches project the target document into a latent, non-interpretable space. The authors' empirical evidence suggests that opaque approaches may be overkill for downstream applications like fake news detection or sentiment analysis, as they add an additional level of complexity without significant performance gain. This observation raises the question of whether it makes sense to explain a black box with another black box.\n",
      "\n",
      "2404.12457v1.RAGCache__Efficient_Knowledge_Caching_for_Retrieval_Augmented_Generation\n",
      "The text discusses a novel multilevel dynamic caching system called RAGCache, designed for efficient knowledge caching in retrieval-augmented generation. RAGCache aims to reduce the computation and memory costs associated with long sequence generation in RAG systems. The system organizes the intermediate states of retrieved knowledge in a knowledge tree and caches them in the GPU and host memory hierarchy. It proposes a replacement policy that is aware of LLM inference characteristics and RAG retrieval patterns, and dynamically overlaps the retrieval and inference steps to minimize the end-to-end latency. Experimental results show that RAGCache reduces the time to first token by up to 4x and improves the throughput by up to 2.1x compared to vLLM integrated with Faiss.\n",
      "\n",
      "2404.15608v1.Understanding_and_Improving_CNNs_with_Complex_Structure_Tensor__A_Biometrics_Study\n",
      "The paper presents a study that demonstrates the effectiveness of using Complex Structure Tensor as an input to Convolutional Neural Networks (CNNs) for biometric identification. This approach improves identification accuracy compared to using grayscale inputs alone. Mini complex conv-nets combined with reduced CNN sizes outperform full-fledged CNN architectures, suggesting that the upfront use of orientation features in CNNs enhances their explainability and relevance to thin-clients. The experiments were conducted on publicly available data sets for biometric identification and verification using 6 State of the Art CNN architectures. The study reduced the Equal Error Rate (EER) on the PolyU dataset by 5-26% depending on data and scenario.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, summary in articles_db.get_all_articles():\n",
    "    print(name)\n",
    "    print(summary)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ff7d5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models.gigachat import GigaChat\n",
    "\n",
    "with open(\"credentials.txt\", 'r', encoding='utf-8') as file:\n",
    "    credentials = file.read()\n",
    "    \n",
    "llm = GigaChat(credentials=credentials, scope=\"GIGACHAT_API_CORP\", verify_ssl_certs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dca57b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgigachat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GigaChatEmbeddings\n\u001b[1;32m      6\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m GigaChatEmbeddings(\n\u001b[1;32m      7\u001b[0m     credentials\u001b[38;5;241m=\u001b[39mcredentials, scope\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGIGACHAT_API_CORP\u001b[39m\u001b[38;5;124m\"\u001b[39m, verify_ssl_certs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m faiss_db \u001b[38;5;241m=\u001b[39m FAISS\u001b[38;5;241m.\u001b[39mfrom_documents(\u001b[43mdocuments\u001b[49m, embedding\u001b[38;5;241m=\u001b[39membeddings)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'documents' is not defined"
     ]
    }
   ],
   "source": [
    "from chromadb.config import Settings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain_community.embeddings.gigachat import GigaChatEmbeddings\n",
    "\n",
    "embeddings = GigaChatEmbeddings(\n",
    "    credentials=credentials, scope=\"GIGACHAT_API_CORP\", verify_ssl_certs=False\n",
    ")\n",
    "\n",
    "faiss_db = FAISS.from_documents(documents, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1afc0c07",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     RecursiveCharacterTextSplitter,\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      6\u001b[0m loader \u001b[38;5;241m=\u001b[39m PyPDFDirectoryLoader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massets/arxiv/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m documents \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter(\n\u001b[1;32m      9\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m     10\u001b[0m     chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m documents \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_documents(documents)\n",
      "File \u001b[0;32m/workspace/Syrenny/GigaHack/Syrenny/giga_venv/lib/python3.10/site-packages/langchain_community/document_loaders/pdf.py:256\u001b[0m, in \u001b[0;36mPyPDFDirectoryLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    255\u001b[0m     loader \u001b[38;5;241m=\u001b[39m PyPDFLoader(\u001b[38;5;28mstr\u001b[39m(i), extract_images\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_images)\n\u001b[0;32m--> 256\u001b[0m     sub_docs \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m sub_docs:\n\u001b[1;32m    258\u001b[0m         doc\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(i)\n",
      "File \u001b[0;32m/workspace/Syrenny/GigaHack/Syrenny/giga_venv/lib/python3.10/site-packages/langchain_core/document_loaders/base.py:29\u001b[0m, in \u001b[0;36mBaseLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m     28\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/Syrenny/GigaHack/Syrenny/giga_venv/lib/python3.10/site-packages/langchain_community/document_loaders/pdf.py:193\u001b[0m, in \u001b[0;36mPyPDFLoader.lazy_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m     blob \u001b[38;5;241m=\u001b[39m Blob\u001b[38;5;241m.\u001b[39mfrom_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/Syrenny/GigaHack/Syrenny/giga_venv/lib/python3.10/site-packages/langchain_core/document_loaders/base.py:125\u001b[0m, in \u001b[0;36mBaseBlobParser.parse\u001b[0;34m(self, blob)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, blob: Blob) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m    111\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Eagerly parse the blob into a document or documents.\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    This is a convenience method for interactive development environment.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m        List of documents\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/Syrenny/GigaHack/Syrenny/giga_venv/lib/python3.10/site-packages/langchain_community/document_loaders/parsers/pdf.py:96\u001b[0m, in \u001b[0;36mPyPDFParser.lazy_parse\u001b[0;34m(self, blob)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m blob\u001b[38;5;241m.\u001b[39mas_bytes_io() \u001b[38;5;28;01mas\u001b[39;00m pdf_file_obj:  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     pdf_reader \u001b[38;5;241m=\u001b[39m pypdf\u001b[38;5;241m.\u001b[39mPdfReader(pdf_file_obj, password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpassword)\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m [\n\u001b[1;32m     97\u001b[0m         Document(\n\u001b[1;32m     98\u001b[0m             page_content\u001b[38;5;241m=\u001b[39mpage\u001b[38;5;241m.\u001b[39mextract_text()\n\u001b[1;32m     99\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_images_from_page(page),\n\u001b[1;32m    100\u001b[0m             metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: blob\u001b[38;5;241m.\u001b[39msource, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage\u001b[39m\u001b[38;5;124m\"\u001b[39m: page_number},  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    101\u001b[0m         )\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m page_number, page \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pdf_reader\u001b[38;5;241m.\u001b[39mpages)\n\u001b[1;32m    103\u001b[0m     ]\n",
      "File \u001b[0;32m/workspace/Syrenny/GigaHack/Syrenny/giga_venv/lib/python3.10/site-packages/langchain_community/document_loaders/parsers/pdf.py:98\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m blob\u001b[38;5;241m.\u001b[39mas_bytes_io() \u001b[38;5;28;01mas\u001b[39;00m pdf_file_obj:  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     pdf_reader \u001b[38;5;241m=\u001b[39m pypdf\u001b[38;5;241m.\u001b[39mPdfReader(pdf_file_obj, password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpassword)\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m [\n\u001b[1;32m     97\u001b[0m         Document(\n\u001b[0;32m---> 98\u001b[0m             page_content\u001b[38;5;241m=\u001b[39m\u001b[43mpage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_images_from_page(page),\n\u001b[1;32m    100\u001b[0m             metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: blob\u001b[38;5;241m.\u001b[39msource, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage\u001b[39m\u001b[38;5;124m\"\u001b[39m: page_number},  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    101\u001b[0m         )\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m page_number, page \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pdf_reader\u001b[38;5;241m.\u001b[39mpages)\n\u001b[1;32m    103\u001b[0m     ]\n",
      "File \u001b[0;32m/workspace/Syrenny/GigaHack/Syrenny/giga_venv/lib/python3.10/site-packages/pypdf/_page.py:2083\u001b[0m, in \u001b[0;36mPageObject.extract_text\u001b[0;34m(self, orientations, space_width, visitor_operand_before, visitor_operand_after, visitor_text, extraction_mode, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2080\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(orientations, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m   2081\u001b[0m     orientations \u001b[38;5;241m=\u001b[39m (orientations,)\n\u001b[0;32m-> 2083\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2084\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2085\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2086\u001b[0m \u001b[43m    \u001b[49m\u001b[43morientations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspace_width\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mPG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCONTENTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvisitor_operand_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvisitor_operand_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvisitor_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2092\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/Syrenny/GigaHack/Syrenny/giga_venv/lib/python3.10/site-packages/pypdf/_page.py:1823\u001b[0m, in \u001b[0;36mPageObject._extract_text\u001b[0;34m(self, obj, pdf, orientations, space_width, content_key, visitor_operand_before, visitor_operand_after, visitor_text)\u001b[0m\n\u001b[1;32m   1821\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(op, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m)):\n\u001b[1;32m   1822\u001b[0m             process_operation(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTj\u001b[39m\u001b[38;5;124m\"\u001b[39m, [op])\n\u001b[0;32m-> 1823\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNumberObject\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFloatObject\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m   1824\u001b[0m             (\u001b[38;5;28mabs\u001b[39m(\u001b[38;5;28mfloat\u001b[39m(op)) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m _space_width)\n\u001b[1;32m   1825\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1826\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m (text[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1827\u001b[0m         ):\n\u001b[1;32m   1828\u001b[0m             process_operation(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTj\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1829\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m operator \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDo\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "loader = PyPDFDirectoryLoader(\"assets/arxiv/\")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "documents = text_splitter.split_documents(documents)\n",
    "print(f\"Total documents: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1933659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm, retriever=db.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "0b5dd5ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Расскажи про Retrieval-Augmented Audio Deepfake Detection',\n",
       " 'result': 'Извините, но я не могу найти информацию о \"Retrieval-Augmented Audio Deepfake Detection\". Можете ли вы уточнить, что именно вы хотите узнать о нем?'}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain({\"query\": \"Расскажи про Retrieval-Augmented Audio Deepfake Detection\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ae2985a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cb8e3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Coroutine, List\n",
    "\n",
    "\n",
    "class HuggingFaceE5Embeddings(HuggingFaceEmbeddings):\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        text = f\"query: {text}\"\n",
    "        return super().embed_query(text)\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        texts = [f\"passage: {text}\" for text in texts]\n",
    "        return super().embed_documents(texts)\n",
    "\n",
    "    async def aembed_query(self, text: str) -> Coroutine[Any, Any, List[float]]:\n",
    "        text = f\"query: {text}\"\n",
    "        return await super().aembed_query(text)\n",
    "\n",
    "    async def aembed_documents(\n",
    "        self, texts: List[str]\n",
    "    ) -> Coroutine[Any, Any, List[List[float]]]:\n",
    "        texts = [f\"passage: {text}\" for text in texts]\n",
    "        return await super().aembed_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf9c196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = HuggingFaceE5Embeddings(model_name=\"intfloat/multilingual-e5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81686a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 66 0 (offset 0)\n",
      "Ignoring wrong pointing object 68 0 (offset 0)\n",
      "Ignoring wrong pointing object 80 0 (offset 0)\n",
      "Ignoring wrong pointing object 85 0 (offset 0)\n",
      "Ignoring wrong pointing object 232 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Ignoring wrong pointing object 34 0 (offset 0)\n",
      "Ignoring wrong pointing object 44 0 (offset 0)\n",
      "Ignoring wrong pointing object 46 0 (offset 0)\n",
      "Ignoring wrong pointing object 55 0 (offset 0)\n",
      "Ignoring wrong pointing object 57 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "\n",
    "loader = PyPDFDirectoryLoader(\"assets/arxiv/\")\n",
    "documents = loader.load()\n",
    "\n",
    "faiss_db = FAISS.from_documents(documents, embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "443f51c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def tokenize(s: str) -> list[str]:\n",
    "    \"\"\"Очень простая функция разбития предложения на слова\"\"\"\n",
    "    return s.lower().translate(str.maketrans(\"\", \"\", string.punctuation)).split(\" \")\n",
    "\n",
    "embedding_retriever = faiss_db.as_retriever(search_kwargs={\"k\": 2})\n",
    "bm25_retriever = BM25Retriever.from_documents(\n",
    "    documents=documents,\n",
    "    preprocess_func=tokenize,\n",
    "    k=3,\n",
    ")\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[embedding_retriever, bm25_retriever],\n",
    "    weights=[0.4, 0.6],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75d8a6b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Что такое вода?',\n",
       " 'result': 'Вода - это прозрачная бесцветная жидкость без запаха, которая составляет большую часть поверхности Земли и является необходимым условием для жизни большинства живых организмов. Она играет важную роль в различных процессах, таких как поддержание температуры, обеспечение питания и поддержание экосистемы. Вода также используется в различных отраслях промышленности, включая производство продуктов питания, текстиля, химических веществ и энергетики.',\n",
       " 'source_documents': [Document(page_content='Hybrid\\nLLM/Rule-based\\nApproaches\\nto\\nBusiness\\nInsights\\nGeneration\\nfrom\\nStructured\\nData\\nAliaksei\\nVertsel\\nand\\nMikhail\\nRumiantsau\\nNarrative\\nBI\\n(\\nteam@narrative.bi\\n)\\nAbstract\\nIn\\nthe\\nfield\\nof\\nbusiness\\ndata\\nanalysis,\\nthe\\nability\\nto\\nextract\\nactionable\\ninsights\\nfrom\\nvast\\nand\\nvaried\\ndatasets\\nis\\nessential\\nfor\\ninformed\\ndecision-making\\nand\\nmaintaining\\na\\ncompetitive\\nedge.\\nTraditional\\nrule-based\\nsystems,\\nwhile\\nreliable,\\noften\\nfall\\nshort\\nwhen\\nfaced\\nwith\\nthe\\ncomplexity\\nand\\ndynamism\\nof\\nmodern\\nbusiness\\ndata.\\nConversely ,\\nArtificial\\nIntelligence\\n(AI)\\nmodels,\\nparticularly\\nLarge\\nLanguage\\nModels\\n(LLMs),\\noffer\\nsignificant\\npotential\\nin\\npattern\\nrecognition\\nand\\npredictive\\nanalytics\\nbut\\ncan\\nlack\\nthe\\nprecision\\nnecessary\\nfor\\nspecific\\nbusiness\\napplications.\\nThis\\npaper\\nexplores\\nthe\\nefficacy\\nof\\nhybrid\\napproaches\\nthat\\nintegrate\\nthe\\nrobustness\\nof\\nrule-based\\nsystems\\nwith\\nthe\\nadaptive\\npower\\nof\\nLLMs\\nin\\ngenerating\\nactionable\\nbusiness\\ninsights.\\nKeywords\\nLarge\\nLanguage\\nModels\\n·\\nQuestion\\nAnswering\\n·\\nNatural\\nLanguage\\nQuery\\n·\\nSQL\\nDatabases\\n·\\nBusiness\\nIntelligence\\n·\\nBenchmark\\n1.\\nIntroduction\\nAs\\norganizations\\ngrapple\\nwith\\nincreasingly\\ncomplex\\nand\\ndiverse\\ndata\\nsets,\\nthe\\ndemand\\nfor\\nadvanced\\ntechniques\\nthat\\ncan\\nextract\\nvaluable\\ninsights\\nhas\\ngrown\\nexponentially .\\nTraditional\\nrule-based\\nsystems\\nhave\\noften\\nstruggled\\nto\\nkeep\\nup\\nwith\\nthe\\nintricacies\\nof\\nmodern\\nbusiness\\ndata,\\nwhile\\nstand-alone\\nAI\\nModels,\\nalthough\\npowerful,\\nmay\\nstill\\nhave\\nlimitations\\nin\\ncertain\\nscenarios.\\nIn\\nresponse\\nto\\nthese\\nchallenges,\\nthe\\nconcept\\nof\\nhybrid\\napproaches\\nhas\\nemer ged\\nas\\na\\ncompelling\\nsolution.\\nBy\\ncombining\\nthe\\nstrengths\\nof\\nrule-based\\nsystems\\nand\\nAI\\nmodels,\\nhybrid\\napproaches\\noffer\\nthe\\npotential\\nto\\nenhance\\nthe\\nprocess\\nof\\ndata\\nextraction\\nand\\nuncover\\nmeaningful\\ninsights\\nfrom\\ndiverse\\ndata\\nsources.\\nIn\\nthis\\npaper ,\\nwe\\nexplore\\nthe\\nuse\\nof\\nLLM-powered\\nand\\nrule-based\\nsystems\\nto\\naddress\\nthe\\ncomplexities\\nof\\ndata\\nextraction\\nin\\nthe\\nfield\\nof\\nbusiness\\nintelligence.', metadata={'source': 'assets/arxiv/2404.15604v1.Hybrid_LLM_Rule_based_Approaches_to_Business_Insights_Generation_from_Structured_Data.pdf', 'page': 0}),\n",
       "  Document(page_content='C.1. GENERATING EXERCISE PLANS (USING INSTRUCTIONS FROM AGAPIE ET\\nAL. [6])\\nC.1.2 GPT-4 exercise plan output:\\nFigure C.1: Exercise plan generated by GPT-4 in response to prompt from Section C.1.1.\\n151', metadata={'source': 'assets/arxiv/2404.13633v1.Incorporating_Different_Verbal_Cues_to_Improve_Text_Based_Computer_Delivered_Health_Messaging.pdf', 'page': 163}),\n",
       "  Document(page_content='APPENDIX C. CHAPTER 6 APPENDICES\\nLikes:\\nRunning, aerobics, dancing, strength training - I like activities with constant motion, and\\nI’m most interested in cardio exercise. Something I can listen to podcasts while doing is also\\nappealing\\nInterested in, but have not tried:\\nMore structured strength training activities - I rarely go to a gym with access to strength\\ntraining equipment. I also don’t know how to use most of that equipment\\nTried, but did not enjoy:\\nYoga - While I see the benefits of it, it did not feel like I was improving my body while doing it\\nOther preferences:\\nI’d prefer to exercise in the morning.\\n150', metadata={'source': 'assets/arxiv/2404.13633v1.Incorporating_Different_Verbal_Cues_to_Improve_Text_Based_Computer_Delivered_Health_Messaging.pdf', 'page': 162}),\n",
       "  Document(page_content='Graphical abstract:  \\n  \\n', metadata={'source': 'assets/arxiv/2402.01733v1.Development_and_Testing_of_Retrieval_Augmented_Generation_in_Large_Language_Models____A_Case_Study_Report.pdf', 'page': 3}),\n",
       "  Document(page_content='14 \\n   ', metadata={'source': 'assets/arxiv/2404.09577v1.Transformers__Contextualism__and_Polysemy.pdf', 'page': 13})]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms.gigachat import GigaChat\n",
    "\n",
    "llm = GigaChat(credentials=credentials, scope=\"GIGACHAT_API_CORP\", verify_ssl_certs=False)\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=ensemble_retriever,\n",
    "    return_source_documents=True,\n",
    ")\n",
    "qa.invoke({\"query\": \"Что такое RAG?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e399ec4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Что такое RAG?',\n",
       " 'result': 'RAG - это аббревиатура от \"Retrieval-Augmented Generation\", что в переводе означает \"Усиление за счет поиска\". Это метод обработки естественного языка, который сочетает в себе преимущества поиска и генерации. Он используется для улучшения качества ответов на вопросы и решения других задач в области обработки естественного языка.',\n",
       " 'source_documents': [Document(page_content='Typos that Broke the RAG’s Back: Genetic Attack on RAG Pipeline\\nby Simulating Documents in the Wild via Low-level Perturbations\\nSukmin Cho Soyeong Jeong Jeongyeon Seo Taeho Hwang Jong C. Park*\\nSchool of Computing\\nKorea Advanced Institute of Science and Technology\\n{nelllpic,starsuzi,yena.seo,doubleyyh,jongpark}@kaist.ac.kr\\nAbstract\\nThe robustness of recent Large Language Mod-\\nels (LLMs) has become increasingly crucial\\nas their applicability expands across various\\ndomains and real-world applications. Retrieval-\\nAugmented Generation (RAG) is a promising\\nsolution for addressing the limitations of LLMs,\\nyet existing studies on the robustness of RAG\\noften overlook the interconnected relationships\\nbetween RAG components or the potential\\nthreats prevalent in real-world databases, such\\nas minor textual errors. In this work, we investi-\\ngate two underexplored aspects when assessing\\nthe robustness of RAG: 1) vulnerability to noisy\\ndocuments through low-level perturbations and\\n2) a holistic evaluation of RAG robustness. Fur-\\nthermore, we introduce a novel attack method,\\nthe Genetic Attack on RAG ( GARAG ), which\\ntargets these aspects. Specifically, GARAG is\\ndesigned to reveal vulnerabilities within each\\ncomponent and test the overall system func-\\ntionality against noisy documents. We validate\\nRAG robustness by applying our GARAG to\\nstandard QA datasets, incorporating diverse re-\\ntrievers and LLMs. The experimental results\\nshow that GARAG consistently achieves high\\nattack success rates. Also, it significantly dev-\\nastates the performance of each component and\\ntheir synergy, highlighting the substantial risk\\nthat minor textual inaccuracies pose in disrupt-\\ning RAG systems in the real world.1\\n1 Introduction\\nRecent Large Language Models (LLMs) (Brown\\net al., 2020; OpenAI, 2023b) have enabled remark-\\nable advances in diverse Natural Language Process-\\ning (NLP) tasks, especially in Question-Answering\\n(QA) tasks (Joshi et al., 2017; Kwiatkowski et al.,\\n2019). Despite these advances, however, LLMs\\nface challenges in having to adapt to ever-evolving\\nor long-tailed knowledge due to their limited para-\\nmetric memory (Kasai et al., 2023; Mallen et al.,\\n*Corresponding author\\n1The code will be released after acceptance.\\nFigure 1: Impact of the noisy document in the real-world\\ndatabase on the RAG system.\\n2023), resulting in a hallucination where the mod-\\nels generate convincing yet factually incorrect\\ntext (Li et al., 2023a). Retrieval-Augmented Gen-\\neration (RAG) (Lewis et al., 2020) has emerged\\nas a promising solution by utilizing a retriever to\\nfetch enriched knowledge from external databases,\\nthus enabling accurate, relevant, and up-to-date re-\\nsponse generation. Specifically, RAG has shown\\nits superior performance across diverse knowledge-\\nintensive tasks (Lewis et al., 2020; Lazaridou et al.,\\n2022; Jeong et al., 2024), leading to its integra-\\ntion as a core component in various real-world\\nAPIs (Qin et al., 2024; Chase, 2022; OpenAI,\\n2023a). Given its extensive applications, ensuring\\nrobustness under diverse conditions of real-world\\nscenarios becomes critical for safe deployment.\\nThus, assessing potential vulnerabilities within the\\noverall RAG system is vital, particularly by assess-\\ning its components: the retriever and the reader.\\nHowever, existing studies on assessing the ro-\\nbustness of RAG often focus solely on either re-\\ntrievers (Zhong et al., 2023; Zou et al., 2024; Long\\net al., 2024) or readers (Li et al., 2023b; Wang et al.,\\n2023; Zhu et al., 2023). The robustness of a single\\ncomponent might only partially capture the com-\\nplexities of RAG systems, where the retriever and\\nreader work together in a sequential flow, which isarXiv:2404.13948v1  [cs.CL]  22 Apr 2024', metadata={'source': 'assets/arxiv/2404.13948v1.Typos_that_Broke_the_RAG_s_Back__Genetic_Attack_on_RAG_Pipeline_by_Simulating_Documents_in_the_Wild_via_Low_level_Perturbations.pdf', 'page': 0}),\n",
       "  Document(page_content='RAGCache: Efficient Knowledge Caching for\\nRetrieval-Augmented Generation\\nChao Jin1Zili Zhang1Xuanlin Jiang1Fangyue Liu1\\nXin Liu2Xuanzhe Liu1Xin Jin1\\n1Peking University2ByteDance Inc.\\nAbstract\\nRetrieval-Augmented Generation (RAG) has shown signifi-\\ncant improvements in various natural language processing\\ntasks by integrating the strengths of large language models\\n(LLMs) and external knowledge databases. However, RAG\\nintroduces long sequence generation and leads to high com-\\nputation and memory costs. We propose RAGCache, a novel\\nmultilevel dynamic caching system tailored for RAG. Our\\nanalysis benchmarks current RAG systems, pinpointing the\\nperformance bottleneck (i.e., long sequence due to knowl-\\nedge injection) and optimization opportunities (i.e., caching\\nknowledge’s intermediate states). Based on these insights, we\\ndesign RAGCache, which organizes the intermediate states\\nof retrieved knowledge in a knowledge tree and caches them\\nin the GPU and host memory hierarchy. RAGCache proposes\\na replacement policy that is aware of LLM inference char-\\nacteristics and RAG retrieval patterns. It also dynamically\\noverlaps the retrieval and inference steps to minimize the\\nend-to-end latency. We implement RAGCache and evaluate\\nit on vLLM, a state-of-the-art LLM inference system and\\nFaiss, a state-of-the-art vector database. The experimental\\nresults show that RAGCache reduces the time to first token\\n(TTFT) by up to 4×and improves the throughput by up to\\n2.1×compared to vLLM integrated with Faiss.\\n1 Introduction\\nRecent advancements in large language models (LLMs) like\\nGPT-4 [ 35], LLaMA2 [ 41], and PalM [ 13] have significantly\\nenhanced performance across various natural language pro-\\ncessing (NLP) tasks, including question answering, summa-\\nrization, and translation [ 39,51,52]. Retrieval-augmented\\ngeneration (RAG) [ 1,27] further enhances LLMs by incor-\\nporating contextually relevant knowledge from external\\ndatabases, such as Wikipedia [ 5], to improve the generation\\nquality. With informative external knowledge, RAG have\\nachieved comparable or even better performance than LLMs\\nfine-tuned for specific downstream tasks [10].\\nFor an RAG request, the RAG system first retrieves rel-\\nevant documents from the knowledge database. The docu-\\nments are typically represented as feature vectors in a vector\\ndatabase through embedding models, and the retrieval step is\\nimplemented by vector similarity search. Then, RAG injects\\nthe retrieved documents (i.e., external knowledge) into the\\noriginal request and feeds the augmented request to the LLMfor generation. With the help of the retrieved documents,\\nRAG expands LLMs’ knowledge base and contextual under-\\nstanding, thereby improving the generation quality [10].\\nWith knowledge injection, RAG introduces long sequence\\ngeneration for the augmented request, which leads to high\\ncomputation and memory costs. For instance, the initial re-\\nquest contains 100 tokens, and the retrieved documents may\\ncontain 1000 tokens in total. Consequently, the extra com-\\nputation and memory costs for the augmented request are\\n>10×higher than the original request. This escalation in re-\\nsource requirements poses a substantial challenge in scaling\\nsystems for efficient processing of RAG requests.\\nRecent work [ 26,57], focusing on system optimizations\\nof LLM inference, has made significant progress in sharing\\nthe intermediate states of LLM inference to reduce recompu-\\ntation costs. vLLM [ 26] manages the intermediate states in\\nnon-contiguous memory blocks to allow fine-grained mem-\\nory allocation and state sharing for a single request’s multi-\\nple generation iterations. SGLang [ 57] identifies the reusable\\nintermediate states across different requests for LLM applica-\\ntions like multi-turn conversations and tree-of-thought [ 48].\\nHowever, these efforts only optimize for LLM inference with-\\nout considering the characteristics of RAG. They cache the\\nintermediate states in GPU memory, which has limited capac-\\nity considering the long sequences in augmented requests,\\nleading to suboptimal performance.\\nWe conduct a system characterization of RAG, which mea-\\nsures the performance of current RAG systems under vari-\\nous datasets and retrieval settings with representative LLMs.\\nOur analysis highlights a significant performance limitation\\nrooted in the processing of augmented sequences due to doc-\\nument injection. In addition, we uncover two potential oppor-\\ntunities for system optimizations to mitigate this constraint.\\nFirst, the recurrence of identical documents across multiple\\nrequests enables the sharing of LLM inference’s intermediate\\nstates for such documents. Second, a small fraction of docu-\\nments accounts for the majority of retrieval requests. This\\nallows us to cache the intermediate states of these frequently\\naccessed documents to reduce the computational burden.\\nTo this end, we propose RAGCache, a novel multilevel\\ndynamic caching system tailored for RAG. RAGCache is the\\nfirst system to cache the intermediate states of retrieved\\ndocuments (i.e., external knowledge) and share them across\\nmultiple requests. The core of RAGCache is a knowledge\\n1arXiv:2404.12457v1  [cs.DC]  18 Apr 2024', metadata={'source': 'assets/arxiv/2404.12457v1.RAGCache__Efficient_Knowledge_Caching_for_Retrieval_Augmented_Generation.pdf', 'page': 0}),\n",
       "  Document(page_content='Evaluating Retrieval Quality in Retrieval-Augmented Generation\\nAlireza Salemi\\nUniversity of Massachusetts Amherst\\nAmherst, MA, United States\\nasalemi@cs.umass.eduHamed Zamani\\nUniversity of Massachusetts Amherst\\nAmherst, MA, United States\\nzamani@cs.umass.edu\\nABSTRACT\\nEvaluating retrieval-augmented generation (RAG) presents chal-\\nlenges, particularly for retrieval models within these systems. Tra-\\nditional end-to-end evaluation methods are computationally expen-\\nsive. Furthermore, evaluation of the retrieval model’s performance\\nbased on query-document relevance labels shows a small correla-\\ntion with the RAG system’s downstream performance. We propose\\na novel evaluation approach, eRAG, where each document in the\\nretrieval list is individually utilized by the large language model\\nwithin the RAG system. The output generated for each document is\\nthen evaluated based on the downstream task ground truth labels.\\nIn this manner, the downstream performance for each document\\nserves as its relevance label. We employ various downstream task\\nmetrics to obtain document-level annotations and aggregate them\\nusing set-based or ranking metrics. Extensive experiments on a\\nwide range of datasets demonstrate that eRAG achieves a higher\\ncorrelation with downstream RAG performance compared to base-\\nline methods, with improvements in Kendall’s 𝜏correlation ranging\\nfrom 0.168 to 0.494. Additionally, eRAG offers significant compu-\\ntational advantages, improving runtime and consuming up to 50\\ntimes less GPU memory than end-to-end evaluation.\\nCCS CONCEPTS\\n•Computing methodologies →Natural language generation ;\\n•Information systems →Evaluation of retrieval results .\\nKEYWORDS\\nEvaluation; Retrieval Quality; Retrieval-Augmented Generation\\nACM Reference Format:\\nAlireza Salemi and Hamed Zamani. 2024. Evaluating Retrieval Quality in\\nRetrieval-Augmented Generation. In Proceedings of the 47th Int’l ACM SIGIR\\nConference on Research and Development in Information Retrieval (SIGIR ’24),\\nJuly 14–18, 2024, Washington, DC, USA. ACM, New York, NY, USA, 6 pages.\\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\\n1 INTRODUCTION\\nRetrieval-augmented generation (RAG) has emerged as a prominent\\napproach in natural language processing, combining the strengths\\nof retrieval and generation models [35], with use cases in decreas-\\ning hallucination [ 1,29], knowledge-grounding [ 9,16,34], and\\npersonalization [ 25,26]. Evaluating RAG systems is important as\\nPermission to make digital or hard copies of part or all of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for third-party components of this work must be honored.\\nFor all other uses, contact the owner/author(s).\\nSIGIR ’24, July 14–18, 2024, Washington, DC, USA.\\n©2024 Copyright held by the owner/author(s).\\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM.\\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnnit ensures the effectiveness of integrating retrieval-based methods\\nwith generative models [ 10,23]. Traditionally, RAG evaluation has\\nprimarily relied on end-to-end assessment, which entails compar-\\ning the generated output with one or more ground truth references\\n[20]. While this is crucial, it presents several limitations, especially,\\nfor evaluating retrieval models in RAG systems.\\nFirst, end-to-end evaluation lacks transparency regarding which\\nretrieved document contributed to the generated output, hindering\\ninterpretability of the system’s behavior. Secondly, it is resource-\\nintensive, consuming significant time and computational power,\\nparticularly when dealing with a large set of retrieval results con-\\nsumed by the LLM. To process long input sequences resulting from\\nthe utilization of all retrieved documents by the LLM, GPUs with\\nsubstantial memory capacities are essential for end-to-end evalu-\\nation. Moreover, many ranking systems rely on interleaving (i.e.,\\nreplacing one or more documents in the result list) for evaluation\\nand optimization, which further complicates the evaluation, as\\nslight variations in retrieval results necessitate re-computation of\\nthe RAG pipeline. Finally, optimizing ranking models often requires\\ndocument-level feedback, such as user clicks [3, 6]. However, end-\\nto-end evaluation only provides list-level feedback for the retrieval\\nresults. That said, this paper studies retrieval evaluation in RAG.\\nHuman annotations can be a potential solution for evaluating\\nretrieval models in RAG, however, accurate annotations are often\\nchallenging and costly to obtain. More recently, with the emergence\\nof large language models (LLMs) and their advanced capabilities\\nin reasoning and text comprehension, they have been utilized to\\nannotate documents for retrieval evaluation [ 10,23]. Nevertheless,\\nthese approaches predominantly evaluate the retriever in RAG sys-\\ntems based on human preferences, whereas the primary objective\\nof the retrieval model in RAG is to serve the LLM that leverages\\nthe retrieved results [ 35]. That said, our extensive investigation on\\na diverse set of RAG systems for open-domain question answer-\\ning, fact verification, and dialogue systems reveals that employing\\nhuman annotations, such as the provenance labels in the KILT bench-\\nmark [ 20], for evaluating the retrieval models within a RAG system\\nexhibits only a minor correlation with the downstream RAG per-\\nformance. This indicates a lack of meaningful relationship between\\nthe evaluated metrics and the downstream performance of RAG.\\nIn this paper, we propose eRAG, a new approach for evaluating\\nretrievers in RAG systems, where we apply the LLM in RAG system\\non each document in the retrieval result list individually and use\\nthe LLM’s output to provide document-level annotations. These\\nannotations can be obtained using any arbitrary downstream task\\nmetric, such as accuracy, exact match, or ROUGE [ 17]. We can then\\napply a set-based or ranking metric as an aggregation function to\\nobtain a single evaluation score for each retrieval result list.\\nWe evaluate our proposed approach on question answering, fact-\\nchecking, and dialogue generation from the knowledge-intensivearXiv:2404.13781v1  [cs.CL]  21 Apr 2024', metadata={'source': 'assets/arxiv/2404.13781v1.Evaluating_Retrieval_Quality_in_Retrieval_Augmented_Generation.pdf', 'page': 0}),\n",
       "  Document(page_content='Graphical abstract:  \\n  \\n', metadata={'source': 'assets/arxiv/2402.01733v1.Development_and_Testing_of_Retrieval_Augmented_Generation_in_Large_Language_Models____A_Case_Study_Report.pdf', 'page': 3}),\n",
       "  Document(page_content='SIGIR ’24, July 14–18, 2024, Washington, DC, USA. Alireza Salemi and Hamed Zamani\\nConference on Empirical Methods in Natural Language Processing , Ellen Riloff,\\nDavid Chiang, Julia Hockenmaier, and Jun’ichi Tsujii (Eds.). Association for\\nComputational Linguistics, Brussels, Belgium, 2369–2380. https://doi.org/10.\\n18653/v1/D18-1259\\n[34] Hamed Zamani and Michael Bendersky. 2024. Stochastic RAG: End-to-End\\nRetrieval-Augmented Generation through Expected Utility Maximization. In\\nProceedings of the 47th Annual International ACM SIGIR Conference on Researchand Development in Information Retrieval (SIGIR ’24) . (to appear).\\n[35] Hamed Zamani, Fernando Diaz, Mostafa Dehghani, Donald Metzler, and Michael\\nBendersky. 2022. Retrieval-Enhanced Machine Learning. In Proceedings of the 45th\\nInternational ACM SIGIR Conference on Research and Development in Information\\nRetrieval (Madrid, Spain) (SIGIR ’22) . Association for Computing Machinery, New\\nYork, NY, USA, 2875–2886. https://doi.org/10.1145/3477495.3531722', metadata={'source': 'assets/arxiv/2404.13781v1.Evaluating_Retrieval_Quality_in_Retrieval_Augmented_Generation.pdf', 'page': 5})]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.invoke({\"query\": \"Что такое RAG?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22d6aa8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Кратко расскажи про особенности RAD',\n",
       " 'result': 'RAD (Retrieval-Augmented Detection) - это метод, который используется для улучшения процесса обнаружения. Он состоит из трех основных этапов: построение базы знаний, поиск знаний и получение результатов. В процессе построения базы знаний, аудиоданные разбиваются на более мелкие фрагменты, которые затем кодируются в латентные представления. Затем эти представления преобразуются в векторные представления, что позволяет проводить эффективный поиск. В процессе поиска знаний, векторные представления используются для поиска наиболее похожих аудиофрагментов. Полученные результаты затем используются для принятия решения о подлинности аудиоданных.',\n",
       " 'source_documents': [Document(page_content='ICMR ’24, June 10–14, 2024, Phuket, Thailand Zuheng Kang et al.\\nFigure 3: The overview of the RAG and RAD pipeline. Triangular edge rectangles represent vectors for retrieval databases.\\nIn RAG, long rectangles represent document chunks. In RAD, long rectangles with/without an outline represent long/short\\nfeatures, rounded edge rectangles represent audio segments.\\nend-to-end manner without the RAD framework. That is, shown\\nin Figure 2, the speech is encoded into short features by a train-\\nable WavLM model Eand time-wise speedup method S, which is\\nthen encoded into intermediate representations by an MFA mod-\\nule. These representations are then classified as either bonafide\\nor spoofed by a fully connected layer. By jointly optimizing the\\nparameters, we obtain a fine-tuned WavLM model that can serve\\nas an improved feature extractor in the subsequent RAD frame-\\nwork. In the subsequent RAD inference phase, we only leverage\\nthe fine-tuned WavLM and discard the back-end model.\\n3.2 Retrieval Augmented Detection\\nTo address the performance limitations imposed by the detection\\nbottleneck, we propose the retrieval augmented detection (RAD)\\nframework. Similar to the RAG framework, the proposed RAD\\napproach consists of three main stages, but with some procedural\\nmodifications compared to RAG.\\n(1) Build Knowledge Retrieval Database. As shown in the stage 1\\n(blue section) of Figure 3-RAD, the bonafide audio dataset 𝑥′is\\nsegmented into smaller audio segments {𝑥𝑛}at the𝑛thsample.\\nThese audio segments can be encoded to latent long feature repre-\\nsentationsn\\n𝑦′\\n𝑛,𝑙o\\n∈R𝑁×𝐿×𝑇′×𝐹by WavLM feature extractor E(·),\\nwhere𝑇′is the time dimension of long features, 𝑁is the number of\\naudio segments, and 𝑙indexes the encoder layer of WavLM. Subse-\\nquently, two operations are performed on the long featuresn\\n𝑦′\\n𝑛,𝑙o\\n:\\n(a) The features are embedded into dense vector representations\\x08\\n𝑣𝑛,𝑙\\t\\n∈R𝑁×𝐿×𝐹via the mappingM(·)in Equation 1, where each\\n𝑣𝑛,𝑙summarizes the time-wise features 𝑦′\\n𝑛,𝑙by temporal averag-\\ning to eliminate the time dimension 𝑇′; (b) The time dimension isshortened to form short feature\\x08\\n𝑦𝑛,𝑙\\t\\n∈R𝑁×𝐿×𝑇×𝐹for improved\\nefficiency by the function S(·)(details in§3.4), where𝑇is the time\\ndimension of short feature.\\n𝑣𝑛,𝑙=M\\x10\\n𝑦′\\n𝑛,𝑙\\x11\\n=1\\n𝑇𝑇∑︁\\n𝑡=1𝑦′\\n𝑛,𝑙\\x0c\\x0c\\x0c𝑡. (1)\\nImportantly, each embedding 𝑣𝑛,𝑙maintains an index linking to\\nits original short feature 𝑦𝑛,𝑙, enabling the retrieval of the original\\naudio segment 𝑥𝑛for the source content. Finally, the collection of\\nembeddings\\x08\\n𝑣𝑛,𝑙\\tare stored in 𝑙vector databasesV𝑙to enable\\nefficient similarity search and retrieval.\\n(2) Retrieve Knowledge. As shown in the stage 2 (red section) of\\nFigure 3-RAD, a sample to be detected ˜𝑥𝑞can be embedded into a\\nquery embedding ˜𝑣𝑞,𝑙∈R𝐿×𝐹by functionE,M, and can be con-\\nverted to short features ˜𝑦𝑞,𝑙∈R𝐿×𝑇×𝐹by functionE,S. This query\\nembedding ˜𝑣𝑞,𝑙is then utilized to perform a similarity search across\\nvector databases – for each layer 𝑙, there is a corresponding vector\\ndatabaseV𝑙to be searched. The top 𝐾most similar embeddings\\x08˜𝑣𝑘,𝑙\\t\\n∈R𝐾×𝐿×𝐹are retrieved, along with their associated short\\nfeatures\\x08˜𝑦𝑘,𝑙\\t\\n∈R𝐾×𝐿×𝑇×𝐹. These𝐾most relevant audio sam-\\nples serve as references for detailed comparison with the sample\\nto be detected. By analyzing the similarities and differences, the\\nauthenticity of the tested samples can be better determined.\\n(3) Get Results (Sample Detection). As shown in stage 3 (green sec-\\ntion) of Figure 3-RAD, our proposed RAD framework requires the\\ntraining of an additional detection model. This model accepts the\\nsamples to be tested as well as the most relevant retrieved samples.\\nSpecifically, the detection model is provided with the query short\\nfeatures ˜𝑦𝑞,𝑙and the top𝐾similar short features\\x08˜𝑦𝑘,𝑙\\t\\nto make the', metadata={'source': 'assets/arxiv/2404.13892v2.Retrieval_Augmented_Audio_Deepfake_Detection.pdf', 'page': 3}),\n",
       "  Document(page_content=\"Retrieval-Augmented Audio Deepfake Detection ICMR ’24, June 10–14, 2024, Phuket, Thailand\\nComparison RAG RADFull training / fine tuning for \\ndetection\\nKnowledge UpdatesUpdates directly  to the latest \\nretrieved knowledge base, \\nwithout the need for frequent re-\\ntraining to obtain the latest \\nknowledge.Knowledge is input implicitly. \\nThe ability of previously \\ntrained models for knowledge \\nacquisition is yet to be verified .Full training or retraining is \\nneeded  for knowledge and data \\nupdates.\\nExternal KnowledgeExternal knowledge can be \\nacquired through retraining .\\nData ProcessingOnly large datasets of high quality \\ncan improve performance.\\nInterpretabilityThe results generated can be \\ntraced back to specific data \\nsources, offering greater \\ninterpretability  and traceability.Only similar samples can be \\ntraced, but it is difficult to \\ninterpret directly  how it relates \\nto the sample being examined.Works as a black box with very \\nlow interpretability .\\nComputational \\nResourcesRelying only on one model's \\njudgment .\\nExternal TrainingZero-shot learning , without \\nexternal training.\\nLatency Requirements Only the model inference process \\nwill have  low latency .\\nHallucination/Detection \\nErrorLess likely to hallucinate because \\neach generated result is based \\non retrieved evidence.Careful comparison and review \\nof retrieved similar samples \\nwith the tested samples may \\nreduce detection errors, but \\nthis needs to be verified .Relying on only one model may \\nproduce higher detection errors .\\nEthical and Privacy \\nIssuesContent in external databases \\nmay have ethical and privacy \\nconcerns .Initially requires full training  for specific tasks with  high quality \\ndata.Utilizing external resources, especially relevant or similar data \\nfrom databases .\\nMinimal data processing and manipulation is required.\\nDatabase retrieval  techniques are required for each generated or \\ndetection task, and  external data sources require regular \\nmaintenance.  These require additional computational resources.\\nData retrieval process leads to higher latency .\\nThe labels are fixed, credible, and free of ethical and privacy \\nconcerns .\\nFigure 4: Properties of RAG, RAD, and full training/fine-\\ntuning for detection. Red text represents the focused atten-\\ntion, and green cells represent ideas that should be verified\\nin this paper.\\nfinal decision 𝑧. Importantly, this detection model not only evaluates\\nrelevant samples, but also provides detailed comparisons with the\\nmost similar real samples. This additional contextual information\\nhelps to make more accurate judgments for DF detections.\\nProperties similar to RAG in RAD. Despite their different applica-\\ntions, with RAD optimized for detection and RAG for generation,\\ngiven the similarities in structure and algorithms between RAG\\nand RAD, it is likely that RAD also has the same advantages as\\nRAG. Figure 4 provides a detailed summary of the key advantages\\nand disadvantages of RAG, RAD, and full training / fine-tuning\\napproaches. Although similarities and differences exist across these\\nmethods, three critical questions emerge as follows:\\n•Question 1: Does the RAD framework reduce detection errors?\\n•Question 2: Does updating external knowledge for the RAD\\nframework further improve detection performance?\\n•Question 3: Can the retrieved audio samples be interpreted?\\nResearch questions 1, 2 are verified in §4.4, and question 3 is\\nverified in§4.5.\\n3.3 Detection Model\\nTo apply RAD to DF detection, we extend the Multi-Fusion Attentive\\n(MFA) classifier [ 9], named RAD-MFA, which combines the raw\\nquery input for detection and the retrieved similar bonafide samples\\nto make comprehensive analysis for detections. Specifically, Figure\\n5 illustrates the overall structure of our proposed detection model,\\nand Figure 5 shows the MFA sub-modules in detail.\\nMFA Module. The MFA Module in our framework handles the test\\nfeature ˜𝑦𝑞and the retrieved features\\x08˜𝑦𝑘,𝑙\\t\\n. For conciseness, these\\nfeatures are denoted by 𝑦in Figure 5. Specifically, the MFA module\\nis implemented through the following steps:\\nFigure 5: The structure of detection model architecture. ⊕\\ndenotes the concatenation. This process illustrates the 3rdget\\nresults stage of Figure 3-RAD in detail.\\n(1)The input feature 𝑦∈R𝐵×𝐿×𝑇×𝐹is passed through 𝐿parallel\\ntime-wise attentive statistic pooling (ASP) layers (denoted as\\nASP𝑇(·)) to eliminate the time dimension. Here, 𝐵denotes a\\nvirtual dimension.\\n(2)The last outputs are concatenated and passed through a fully\\nconnected layer to transform the features to R𝐵×𝐿×2𝐹.\\n(3)These outputs are then passed through a layer-wise ASP layer\\n(denoted as ASP𝐿(·)) to form the intermediate representation\\n𝑟∈R𝐵×4𝐹.\\nExtended RAD-based MFA. The RAD-MFA is implemented through\\nthe following steps:\\n(1)The test feature ˜𝑦𝑞∈R1×𝐿×𝑇×𝐹and the retrieved features\\x08˜𝑦𝑘,𝑙\\t\\n∈R𝐾×𝐿×𝑇×𝐹are sent to the same MFA module, creat-\\ning intermediate representations 𝑟𝑞∈R1×4𝐹and𝑟𝑘∈R1×4𝐹\\nrespectively.\\n(2)These two representations are used to form 𝑟𝑑∈R1×4𝐹by\\ntaking their difference, 𝑟𝑑=𝑟𝑘−𝑟𝑞. We make a difference\\nbetween two features with extremely similar timbre, which\\nallows the discriminative model to pay more attention to other\\ndifferential information, such as background noise.\\n(3)This output is sent to a sample-wise ASP layer (denoted as\\nASP𝐾(·)) to form the intermediate representation 𝑟𝑒∈R1×8𝐹.\\n(4)𝑟𝑒is concatenated with 𝑟𝑞, and sent to a fully connected layer\\nto make the final decision.\\nThrough this scheme, the RAD-based detection model can take\\ninto account numerous particularly similar bonafide samples and\\nmake comprehensive judgments on their contents and distributions.\\nThis enables the model to achieve more accurate detection results\\nby accounting for many additional highly similar authentic cases.\\n3.4 Performance Optimization\\nIn order to speed up the process of training and testing, two ap-\\nproaches are used for optimization.\\nLocally Stored Features. To speed up the training and testing pro-\\ncess, we pre-compute and cache the WavLM features of all audio\", metadata={'source': 'assets/arxiv/2404.13892v2.Retrieval_Augmented_Audio_Deepfake_Detection.pdf', 'page': 4}),\n",
       "  Document(page_content='Retrieval-Augmented Audio Deepfake Detection ICMR ’24, June 10–14, 2024, Phuket, Thailand\\nTable 1: Comparison with other anti-spoofing systems in the\\nASVspoof 2019 LA evaluation set, reported in terms of pooled\\nmin t-DCF and EER (%).\\nSystem Configuration min t-DCF EER(%)\\nHua et al. [11] DNN+ResNet 0.0481 1.64\\nZhang et al. [31] FFT+SENet 0.0368 1.14\\nDing et al. [6] SAMO 0.0356 1.08\\nTak et al. [22] RawGAT-ST 0.0335 1.06\\nJung et al. [26] AASIST 0.0275 0.83\\nHuang et al. [12] DFSincNet 0.0176 0.52\\nFan et al. [7] f0+Res2Net 0.0159 0.47\\nGuo et al. [9] WavLM+MFA 0.0126 0.42\\nOurs WavLM+RAD-MFA 0.0115 0.40\\nModel Training. The front-end feature extractor utilized in this\\nwork is WavLM. During fine-tuning of the front-end WavLM, the\\nAdam optimizer is employed with a learning rate of 3e-6 and a batch\\nsize of 4. For training the MFA, the batch size is changed to 32 and\\nthe learning rate is 3e-5. All experiments were performed utilizing\\ntwo NVIDIA GeForce RTX 3090 GPUs. Each model configuration\\nis trained for approximately 30 epochs.\\n4.3 Experimental Results\\nTo demonstrate the superior performance of our proposed method\\nover existing approaches, we compare our proposed method to\\nrecent SOTA methods.\\nResults on ASVspoof 2019 LA evaluation set. The experimental re-\\nsults in Table 1 compare the performance of our proposed methods\\nto existing approaches on the ASVspoof 2019 LA evaluation dataset.\\nOur method achieves an EER of 0.40% and a min t-DCF of 0.0115\\nwhich is the best reporting result, demonstrating the effectiveness\\nand superiority of our proposed method. Notably, although Guo\\net al. [ 9] utilizes a similar WavLM feature extractor and MFA net-\\nwork, our proposed RAD framework improves its performance,\\novercoming the limitations of single-model approaches.\\nIn our analysis, the RAD framework first retrieves the most sim-\\nilar audio samples, which are likely from the same speaker, and\\nthen performs careful comparisons between these samples and the\\ntest sample. For the detection model, it only needs to consider the\\ndifferences between the two, rather than relying on fuzzy prior\\nknowledge for detection. In contrast, our proposed method is more\\nrobust. Specifically, by focusing on fine-grained differences rather\\nthan generalized knowledge, our method can more accurately dis-\\ntinguish more detailed information.\\nResults on ASVspoof 2021 DF evaluation set. We further test our\\nmodel on the ASVspoof 2021 LA and DF evaluation set, results are\\nshown in Table 2. In the DF subset, our method achieves SOTA\\nperformance on the DF subset with an EER of 2.38%. In the LA\\nsubset, we obtain an EER of 4.89%, which is also quite a competitive\\nperformance, but still better than the baseline system [ 9] without\\nRAD. Further analysis and ablation studies are needed to a fullyTable 2: Comparative results of our proposed method with\\nother systems in the ASVspoof 2021 LA and DF evaluation\\nset with pooled EER (%).\\nSystem Configuration LA DF\\nFan et al. [7] f0+Res2Net 3.61 –\\nDoñas et al. [18] wav2vec2+ASP 3.54 4.98\\nWang et al. [25] wav2vec2+LGF 6.53 4.75\\nTak et al. [21] wav2vec2+AASIST 0.82 2.85\\nFan et al. [7] WavLM+MFA 5.08 2.56\\nOurs WavLM+RAD-MFA 4.83 2.38\\nTable 3: Ablation studies on ASVspoof 2021 DF dataset for\\nthe effectiveness of each component with pooled EER (%). -L\\nand -S: large and small. ft: fine-tuning. Just Difference is the\\n𝑟𝑒(denoted in Figure 5, without 𝑟𝑞) directly connected to the\\nfully connected layer for classification.\\nAblation Configuration Pooled EER(%)\\nFull Framework – 2.38\\nw/o RAD Baseline (Figure 2) 2.90\\nw/o VCTK ASVspoof 2019 only 2.54\\nw/o WavLM-L WavLM-S 9.15\\nw/o ftWavLM-S 9.62\\nWavLM-L 4.98\\nVariation Structure Just Difference 2.49\\ncharacterize the advantages of our proposed method on each com-\\nponent.\\n4.4 Ablation Study\\nAblation Study on Different Components. The ablation study pre-\\nsented in Table 3 summarizes the results obtained by evaluating\\ndifferent configurations and components of the proposed system\\non the ASVspoof 2021 DF subset. Specifically, the impact of the\\nRAD framework, WavLM-L feature extractor, fine-tuning of the\\nfeature extractor, incorporation of additional VCTK datasets for\\ndata retrieval, and the structure of the detection network were ana-\\nlyzed. The experiments were conducted using a time-wise speedup\\nparameter𝜏=10. System performance was assessed using the\\npooled EER expressed as a percentage. The key observations are\\nsummarized as follows (line-by-line explanation from Table 3):\\n(1)The full system reaches the SOTA performance with a pooled\\nEER of 2.38%.\\n(2)Removing the proposed RAD framework for similar sample\\nretrieval increases the pooled EER to 2.90%. This validates the\\neffectiveness of the RAD framework, which answers the re-\\nsearch Question 1 . However, this result is slightly higher than\\nthat of Guo et al. [ 9], which may be due to different parameter\\nsettings and time-wise speedup operations.', metadata={'source': 'assets/arxiv/2404.13892v2.Retrieval_Augmented_Audio_Deepfake_Detection.pdf', 'page': 6}),\n",
       "  Document(page_content='Graphical abstract:  \\n  \\n', metadata={'source': 'assets/arxiv/2402.01733v1.Development_and_Testing_of_Retrieval_Augmented_Generation_in_Large_Language_Models____A_Case_Study_Report.pdf', 'page': 3}),\n",
       "  Document(page_content='24 \\n  \\nExhibit 18 - Sample Prompt s \\n \\n \\nExhibit 19 - Micro Prompt output  \\n \\n \\nExhibit 20 - Macro Prompt output  \\n', metadata={'source': 'assets/arxiv/2403.10482v2.Can_a_GPT4_Powered_AI_Agent_Be_a_Good_Enough_Performance_Attribution_Analyst_.pdf', 'page': 23})]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.invoke({\"query\": \"Кратко расскажи про особенности RAD\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f3a4996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Представь несколько идей по использованию RAG системы',\n",
       " 'result': 'RAG система может быть использована для улучшения качества и точности генерации текста в различных областях, таких как вопросы и ответы, факт-чекинг и диалоговое взаимодействие. Она может быть полезна для создания более релевантных и точных ответов на вопросы, а также для проверки фактов и обеспечения достоверности информации. Кроме того, RAG система может быть использована для улучшения персонализации и адаптации контента к потребностям и интересам пользователей.',\n",
       " 'source_documents': [Document(page_content='Typos that Broke the RAG’s Back: Genetic Attack on RAG Pipeline\\nby Simulating Documents in the Wild via Low-level Perturbations\\nSukmin Cho Soyeong Jeong Jeongyeon Seo Taeho Hwang Jong C. Park*\\nSchool of Computing\\nKorea Advanced Institute of Science and Technology\\n{nelllpic,starsuzi,yena.seo,doubleyyh,jongpark}@kaist.ac.kr\\nAbstract\\nThe robustness of recent Large Language Mod-\\nels (LLMs) has become increasingly crucial\\nas their applicability expands across various\\ndomains and real-world applications. Retrieval-\\nAugmented Generation (RAG) is a promising\\nsolution for addressing the limitations of LLMs,\\nyet existing studies on the robustness of RAG\\noften overlook the interconnected relationships\\nbetween RAG components or the potential\\nthreats prevalent in real-world databases, such\\nas minor textual errors. In this work, we investi-\\ngate two underexplored aspects when assessing\\nthe robustness of RAG: 1) vulnerability to noisy\\ndocuments through low-level perturbations and\\n2) a holistic evaluation of RAG robustness. Fur-\\nthermore, we introduce a novel attack method,\\nthe Genetic Attack on RAG ( GARAG ), which\\ntargets these aspects. Specifically, GARAG is\\ndesigned to reveal vulnerabilities within each\\ncomponent and test the overall system func-\\ntionality against noisy documents. We validate\\nRAG robustness by applying our GARAG to\\nstandard QA datasets, incorporating diverse re-\\ntrievers and LLMs. The experimental results\\nshow that GARAG consistently achieves high\\nattack success rates. Also, it significantly dev-\\nastates the performance of each component and\\ntheir synergy, highlighting the substantial risk\\nthat minor textual inaccuracies pose in disrupt-\\ning RAG systems in the real world.1\\n1 Introduction\\nRecent Large Language Models (LLMs) (Brown\\net al., 2020; OpenAI, 2023b) have enabled remark-\\nable advances in diverse Natural Language Process-\\ning (NLP) tasks, especially in Question-Answering\\n(QA) tasks (Joshi et al., 2017; Kwiatkowski et al.,\\n2019). Despite these advances, however, LLMs\\nface challenges in having to adapt to ever-evolving\\nor long-tailed knowledge due to their limited para-\\nmetric memory (Kasai et al., 2023; Mallen et al.,\\n*Corresponding author\\n1The code will be released after acceptance.\\nFigure 1: Impact of the noisy document in the real-world\\ndatabase on the RAG system.\\n2023), resulting in a hallucination where the mod-\\nels generate convincing yet factually incorrect\\ntext (Li et al., 2023a). Retrieval-Augmented Gen-\\neration (RAG) (Lewis et al., 2020) has emerged\\nas a promising solution by utilizing a retriever to\\nfetch enriched knowledge from external databases,\\nthus enabling accurate, relevant, and up-to-date re-\\nsponse generation. Specifically, RAG has shown\\nits superior performance across diverse knowledge-\\nintensive tasks (Lewis et al., 2020; Lazaridou et al.,\\n2022; Jeong et al., 2024), leading to its integra-\\ntion as a core component in various real-world\\nAPIs (Qin et al., 2024; Chase, 2022; OpenAI,\\n2023a). Given its extensive applications, ensuring\\nrobustness under diverse conditions of real-world\\nscenarios becomes critical for safe deployment.\\nThus, assessing potential vulnerabilities within the\\noverall RAG system is vital, particularly by assess-\\ning its components: the retriever and the reader.\\nHowever, existing studies on assessing the ro-\\nbustness of RAG often focus solely on either re-\\ntrievers (Zhong et al., 2023; Zou et al., 2024; Long\\net al., 2024) or readers (Li et al., 2023b; Wang et al.,\\n2023; Zhu et al., 2023). The robustness of a single\\ncomponent might only partially capture the com-\\nplexities of RAG systems, where the retriever and\\nreader work together in a sequential flow, which isarXiv:2404.13948v1  [cs.CL]  22 Apr 2024', metadata={'source': 'assets/arxiv/2404.13948v1.Typos_that_Broke_the_RAG_s_Back__Genetic_Attack_on_RAG_Pipeline_by_Simulating_Documents_in_the_Wild_via_Low_level_Perturbations.pdf', 'page': 0}),\n",
       "  Document(page_content='RAGCache: Efficient Knowledge Caching for\\nRetrieval-Augmented Generation\\nChao Jin1Zili Zhang1Xuanlin Jiang1Fangyue Liu1\\nXin Liu2Xuanzhe Liu1Xin Jin1\\n1Peking University2ByteDance Inc.\\nAbstract\\nRetrieval-Augmented Generation (RAG) has shown signifi-\\ncant improvements in various natural language processing\\ntasks by integrating the strengths of large language models\\n(LLMs) and external knowledge databases. However, RAG\\nintroduces long sequence generation and leads to high com-\\nputation and memory costs. We propose RAGCache, a novel\\nmultilevel dynamic caching system tailored for RAG. Our\\nanalysis benchmarks current RAG systems, pinpointing the\\nperformance bottleneck (i.e., long sequence due to knowl-\\nedge injection) and optimization opportunities (i.e., caching\\nknowledge’s intermediate states). Based on these insights, we\\ndesign RAGCache, which organizes the intermediate states\\nof retrieved knowledge in a knowledge tree and caches them\\nin the GPU and host memory hierarchy. RAGCache proposes\\na replacement policy that is aware of LLM inference char-\\nacteristics and RAG retrieval patterns. It also dynamically\\noverlaps the retrieval and inference steps to minimize the\\nend-to-end latency. We implement RAGCache and evaluate\\nit on vLLM, a state-of-the-art LLM inference system and\\nFaiss, a state-of-the-art vector database. The experimental\\nresults show that RAGCache reduces the time to first token\\n(TTFT) by up to 4×and improves the throughput by up to\\n2.1×compared to vLLM integrated with Faiss.\\n1 Introduction\\nRecent advancements in large language models (LLMs) like\\nGPT-4 [ 35], LLaMA2 [ 41], and PalM [ 13] have significantly\\nenhanced performance across various natural language pro-\\ncessing (NLP) tasks, including question answering, summa-\\nrization, and translation [ 39,51,52]. Retrieval-augmented\\ngeneration (RAG) [ 1,27] further enhances LLMs by incor-\\nporating contextually relevant knowledge from external\\ndatabases, such as Wikipedia [ 5], to improve the generation\\nquality. With informative external knowledge, RAG have\\nachieved comparable or even better performance than LLMs\\nfine-tuned for specific downstream tasks [10].\\nFor an RAG request, the RAG system first retrieves rel-\\nevant documents from the knowledge database. The docu-\\nments are typically represented as feature vectors in a vector\\ndatabase through embedding models, and the retrieval step is\\nimplemented by vector similarity search. Then, RAG injects\\nthe retrieved documents (i.e., external knowledge) into the\\noriginal request and feeds the augmented request to the LLMfor generation. With the help of the retrieved documents,\\nRAG expands LLMs’ knowledge base and contextual under-\\nstanding, thereby improving the generation quality [10].\\nWith knowledge injection, RAG introduces long sequence\\ngeneration for the augmented request, which leads to high\\ncomputation and memory costs. For instance, the initial re-\\nquest contains 100 tokens, and the retrieved documents may\\ncontain 1000 tokens in total. Consequently, the extra com-\\nputation and memory costs for the augmented request are\\n>10×higher than the original request. This escalation in re-\\nsource requirements poses a substantial challenge in scaling\\nsystems for efficient processing of RAG requests.\\nRecent work [ 26,57], focusing on system optimizations\\nof LLM inference, has made significant progress in sharing\\nthe intermediate states of LLM inference to reduce recompu-\\ntation costs. vLLM [ 26] manages the intermediate states in\\nnon-contiguous memory blocks to allow fine-grained mem-\\nory allocation and state sharing for a single request’s multi-\\nple generation iterations. SGLang [ 57] identifies the reusable\\nintermediate states across different requests for LLM applica-\\ntions like multi-turn conversations and tree-of-thought [ 48].\\nHowever, these efforts only optimize for LLM inference with-\\nout considering the characteristics of RAG. They cache the\\nintermediate states in GPU memory, which has limited capac-\\nity considering the long sequences in augmented requests,\\nleading to suboptimal performance.\\nWe conduct a system characterization of RAG, which mea-\\nsures the performance of current RAG systems under vari-\\nous datasets and retrieval settings with representative LLMs.\\nOur analysis highlights a significant performance limitation\\nrooted in the processing of augmented sequences due to doc-\\nument injection. In addition, we uncover two potential oppor-\\ntunities for system optimizations to mitigate this constraint.\\nFirst, the recurrence of identical documents across multiple\\nrequests enables the sharing of LLM inference’s intermediate\\nstates for such documents. Second, a small fraction of docu-\\nments accounts for the majority of retrieval requests. This\\nallows us to cache the intermediate states of these frequently\\naccessed documents to reduce the computational burden.\\nTo this end, we propose RAGCache, a novel multilevel\\ndynamic caching system tailored for RAG. RAGCache is the\\nfirst system to cache the intermediate states of retrieved\\ndocuments (i.e., external knowledge) and share them across\\nmultiple requests. The core of RAGCache is a knowledge\\n1arXiv:2404.12457v1  [cs.DC]  18 Apr 2024', metadata={'source': 'assets/arxiv/2404.12457v1.RAGCache__Efficient_Knowledge_Caching_for_Retrieval_Augmented_Generation.pdf', 'page': 0}),\n",
       "  Document(page_content='Evaluating Retrieval Quality in Retrieval-Augmented Generation\\nAlireza Salemi\\nUniversity of Massachusetts Amherst\\nAmherst, MA, United States\\nasalemi@cs.umass.eduHamed Zamani\\nUniversity of Massachusetts Amherst\\nAmherst, MA, United States\\nzamani@cs.umass.edu\\nABSTRACT\\nEvaluating retrieval-augmented generation (RAG) presents chal-\\nlenges, particularly for retrieval models within these systems. Tra-\\nditional end-to-end evaluation methods are computationally expen-\\nsive. Furthermore, evaluation of the retrieval model’s performance\\nbased on query-document relevance labels shows a small correla-\\ntion with the RAG system’s downstream performance. We propose\\na novel evaluation approach, eRAG, where each document in the\\nretrieval list is individually utilized by the large language model\\nwithin the RAG system. The output generated for each document is\\nthen evaluated based on the downstream task ground truth labels.\\nIn this manner, the downstream performance for each document\\nserves as its relevance label. We employ various downstream task\\nmetrics to obtain document-level annotations and aggregate them\\nusing set-based or ranking metrics. Extensive experiments on a\\nwide range of datasets demonstrate that eRAG achieves a higher\\ncorrelation with downstream RAG performance compared to base-\\nline methods, with improvements in Kendall’s 𝜏correlation ranging\\nfrom 0.168 to 0.494. Additionally, eRAG offers significant compu-\\ntational advantages, improving runtime and consuming up to 50\\ntimes less GPU memory than end-to-end evaluation.\\nCCS CONCEPTS\\n•Computing methodologies →Natural language generation ;\\n•Information systems →Evaluation of retrieval results .\\nKEYWORDS\\nEvaluation; Retrieval Quality; Retrieval-Augmented Generation\\nACM Reference Format:\\nAlireza Salemi and Hamed Zamani. 2024. Evaluating Retrieval Quality in\\nRetrieval-Augmented Generation. In Proceedings of the 47th Int’l ACM SIGIR\\nConference on Research and Development in Information Retrieval (SIGIR ’24),\\nJuly 14–18, 2024, Washington, DC, USA. ACM, New York, NY, USA, 6 pages.\\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\\n1 INTRODUCTION\\nRetrieval-augmented generation (RAG) has emerged as a prominent\\napproach in natural language processing, combining the strengths\\nof retrieval and generation models [35], with use cases in decreas-\\ning hallucination [ 1,29], knowledge-grounding [ 9,16,34], and\\npersonalization [ 25,26]. Evaluating RAG systems is important as\\nPermission to make digital or hard copies of part or all of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for third-party components of this work must be honored.\\nFor all other uses, contact the owner/author(s).\\nSIGIR ’24, July 14–18, 2024, Washington, DC, USA.\\n©2024 Copyright held by the owner/author(s).\\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM.\\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnnit ensures the effectiveness of integrating retrieval-based methods\\nwith generative models [ 10,23]. Traditionally, RAG evaluation has\\nprimarily relied on end-to-end assessment, which entails compar-\\ning the generated output with one or more ground truth references\\n[20]. While this is crucial, it presents several limitations, especially,\\nfor evaluating retrieval models in RAG systems.\\nFirst, end-to-end evaluation lacks transparency regarding which\\nretrieved document contributed to the generated output, hindering\\ninterpretability of the system’s behavior. Secondly, it is resource-\\nintensive, consuming significant time and computational power,\\nparticularly when dealing with a large set of retrieval results con-\\nsumed by the LLM. To process long input sequences resulting from\\nthe utilization of all retrieved documents by the LLM, GPUs with\\nsubstantial memory capacities are essential for end-to-end evalu-\\nation. Moreover, many ranking systems rely on interleaving (i.e.,\\nreplacing one or more documents in the result list) for evaluation\\nand optimization, which further complicates the evaluation, as\\nslight variations in retrieval results necessitate re-computation of\\nthe RAG pipeline. Finally, optimizing ranking models often requires\\ndocument-level feedback, such as user clicks [3, 6]. However, end-\\nto-end evaluation only provides list-level feedback for the retrieval\\nresults. That said, this paper studies retrieval evaluation in RAG.\\nHuman annotations can be a potential solution for evaluating\\nretrieval models in RAG, however, accurate annotations are often\\nchallenging and costly to obtain. More recently, with the emergence\\nof large language models (LLMs) and their advanced capabilities\\nin reasoning and text comprehension, they have been utilized to\\nannotate documents for retrieval evaluation [ 10,23]. Nevertheless,\\nthese approaches predominantly evaluate the retriever in RAG sys-\\ntems based on human preferences, whereas the primary objective\\nof the retrieval model in RAG is to serve the LLM that leverages\\nthe retrieved results [ 35]. That said, our extensive investigation on\\na diverse set of RAG systems for open-domain question answer-\\ning, fact verification, and dialogue systems reveals that employing\\nhuman annotations, such as the provenance labels in the KILT bench-\\nmark [ 20], for evaluating the retrieval models within a RAG system\\nexhibits only a minor correlation with the downstream RAG per-\\nformance. This indicates a lack of meaningful relationship between\\nthe evaluated metrics and the downstream performance of RAG.\\nIn this paper, we propose eRAG, a new approach for evaluating\\nretrievers in RAG systems, where we apply the LLM in RAG system\\non each document in the retrieval result list individually and use\\nthe LLM’s output to provide document-level annotations. These\\nannotations can be obtained using any arbitrary downstream task\\nmetric, such as accuracy, exact match, or ROUGE [ 17]. We can then\\napply a set-based or ranking metric as an aggregation function to\\nobtain a single evaluation score for each retrieval result list.\\nWe evaluate our proposed approach on question answering, fact-\\nchecking, and dialogue generation from the knowledge-intensivearXiv:2404.13781v1  [cs.CL]  21 Apr 2024', metadata={'source': 'assets/arxiv/2404.13781v1.Evaluating_Retrieval_Quality_in_Retrieval_Augmented_Generation.pdf', 'page': 0}),\n",
       "  Document(page_content='Graphical abstract:  \\n  \\n', metadata={'source': 'assets/arxiv/2402.01733v1.Development_and_Testing_of_Retrieval_Augmented_Generation_in_Large_Language_Models____A_Case_Study_Report.pdf', 'page': 3}),\n",
       "  Document(page_content='contingent upon further refinement and validation.\\nIn conclusion, the feedback from experts provides\\na clear directive towards enhancing RAGTAG with\\nspecific focus on improving translation accuracy, in-\\nput data quality, system architecture integration, and\\nleveraging experienced tester insights for refinement.\\nV. T HREATS TO VALIDITY\\nInternal Validity. Bias is one of the main concerns\\nfor internal validity. To mitigate bias, the experts\\nin RQ2 had no exposure to our technical approach.\\nFurther, there was a possibility of social desirability\\nbias, where experts might refrain from expressing dis-\\nsenting opinions—we implemented several strategies\\nto counteract this. We provided opportunities for each\\nexpert to voice their opinions, asked them to verbalize\\ntheir rationales, and randomly alternated the sequence\\nin which they reviewed the TSs. The experts did note\\ntheir disagreement in some cases (Section IV-F).\\nExternal Validity. Generalizability is always a con-\\ncern with an industry-innovation study like ours. We\\nevaluated RAGTAG on two different projects, but\\nmore diverse case studies at Austrian Post and beyond\\nare required to ensure that RAGTAG is effective at\\ngenerating TSs from NL requirements. We further\\nplan to conduct more user studies, wherein the experts\\ncan execute the generated TSsin practice to substan-\\ntiate the results reflected by our evaluation criteria.\\nVI. C ONCLUSION\\nIn this paper, we presented an automated ap-\\nproach ( RAGTAG ) for generating test scenarios from\\nNL requirements, developed in collaboration with\\nAustrian Post Group IT – a division of Austrian\\nPost. RAGTAG is based on large language models\\n(LLMs) and retrieval augmented generation (RAG)\\ntechniques. We reported on the evaluation of RAG-\\nTAG on two projects from Austrian Post, as a\\npart of the LIEF POST software system used for\\npostage and delivery management with bilingual re-\\nquirements, specified in German and English. In\\nthis context, we evaluated eight configurations for\\nRAGTAG and identified the best configuration, with\\nGPT-3.5 LLM and few-shot prompting. The paper\\nfurther reports on an interview survey conducted with\\nfour experts from Austrian Post on the usefulness of\\nRAGTAG in practice, in terms of relevance, coverage,\\ncorrectness, coherence and feasibility of the generated\\ntest scenarios. The results show that experts (strongly)\\nagree that the generated test scenarios are coherent,\\nfeasible, relevant and cover the relevant concepts.\\nThey also indicated the need for a human expert in\\nthe loop to improve correctness. RAGTAG according\\nto the experts is ready for adoption in their context\\nand is likely to save time in their quality assurance\\nefforts. In future, we would like to conduct wider\\nstudies at Austrian Post and beyond to establish thegeneralizability of RAGTAG , and extend the domain\\ndocumentation to include the architectural informa-\\ntion of the systems. We also plan to attempt wider\\nexperiments with more LLMs and RAG parameter\\nsettings, e.g., temperature in LLMs.\\nAcknowledgement. We gratefully acknowledge the\\nefforts invested by the colleagues at Austrian Post, in\\nproviding feedback at different stages in this project.\\nREFERENCES\\n[1] S. Desikan and G. Ramesh, Software testing: principles and\\npractice . Pearson Education India, 2006.\\n[2] L. Crispin and J. Gregory, Agile testing: A practical guide\\nfor testers and agile teams . Pearson Education, 2009.\\n[3] M. Unterkalmsteiner, R. Feldt, and T. Gorschek, “A taxon-\\nomy for requirements engineering and software test align-\\nment,” ACM Transactions on Software Engineering and\\nMethodology (TOSEM) , vol. 23, no. 2, pp. 1–38, 2014.\\n[4] M. Gordon and D. Harel, “Generating executable scenar-\\nios from natural language,” in International Conference on\\nIntelligent Text Processing and Computational Linguistics .\\nSpringer, 2009, pp. 456–467.\\n[5] A. Nayak and D. Samanta, “Synthesis of test scenarios\\nusing uml activity diagrams,” Software & Systems Modeling ,\\nvol. 10, no. 1, pp. 63–89, 2011.\\n[6] S. J. Cunning and J. Rozenbiit, “Test scenario generation\\nfrom a structured requirements specification,” in Proceedings\\nECBS’99. IEEE Conference and Workshop on Engineering of\\nComputer-Based Systems . IEEE, 1999, pp. 166–172.\\n[7] M. Klischat and M. Althoff, “Generating critical test sce-\\nnarios for automated vehicles with evolutionary algorithms,”\\nin2019 IEEE Intelligent Vehicles Symposium (IV) . IEEE,\\n2019, pp. 2352–2358.\\n[8] A. Bertolino, A. Fantechi, S. Gnesi, and G. Lami, “Product\\nline use cases: Scenario-based specification and testing of\\nrequirements,” in Software Product Lines . Springer, 2006,\\npp. 425–445.\\n[9] C. Wang, F. Pastore, A. Goknil, and L. C. Briand, “Au-\\ntomatic generation of acceptance test cases from use case\\nspecifications: an nlp-based approach,” IEEE Transactions on\\nSoftware Engineering , vol. 48, no. 2, pp. 585–616, 2020.\\n[10] B. Hois, S. Sobernig, and M. Strembeck, “Natural-language\\nscenario descriptions for testing core language models of\\ndomain-specific languages,” in 2014 2nd International Con-\\nference on Model-Driven Engineering and Software Devel-\\nopment (MODELSWARD) . IEEE, 2014, pp. 356–367.\\n[11] J. Cem Kaner, “An introduction to scenario testing,” Florida\\nInstitute of Technology, Melbourne , pp. 1–13, 2013.\\n[12] C. Arora, J. Grundy, and M. Abdelrazek, “Advancing require-\\nments engineering through generative ai: Assessing the role\\nof llms,” arXiv preprint arXiv:2310.13976 , 2023.\\n[13] N. Kandpal, H. Deng, A. Roberts, E. Wallace, and C. Raffel,\\n“Large language models struggle to learn long-tail knowl-\\nedge,” in International Conference on Machine Learning .\\nPMLR, 2023, pp. 15 696–15 707.\\n[14] P. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin,\\nN. Goyal, H. K ¨uttler, M. Lewis, W.-t. Yih, T. Rockt ¨aschel\\net al. , “Retrieval-augmented generation for knowledge-\\nintensive nlp tasks,” Advances in Neural Information Pro-\\ncessing Systems , vol. 33, pp. 9459–9474, 2020.\\n[15] Y . Gao, Y . Xiong, X. Gao, K. Jia, J. Pan, Y . Bi, Y . Dai, J. Sun,\\nand H. Wang, “Retrieval-augmented generation for large lan-\\nguage models: A survey,” arXiv preprint arXiv:2312.10997 ,\\n2023.\\n[16] M. Sabetzadeh and C. Arora, “Practical guidelines for the\\nselection and evaluation of NLP techniques in RE,” arXiv\\npreprint arXiv:2401.01508 , 2024.\\n[17] A. Nguyen-Duc, B. Cabrero-Daniel, A. Przybylek, C. Arora,\\nD. Khanna, T. Herda, U. Rafiq, J. Melegati, E. Guerra,\\nK.-K. Kemell et al. , “Generative artificial intelligence for\\nsoftware engineering–a research agenda,” arXiv preprint\\narXiv:2310.18648 , 2023.\\n11', metadata={'source': 'assets/arxiv/2404.12772v1.Generating_Test_Scenarios_from_NL_Requirements_using_Retrieval_Augmented_LLMs__An_Industrial_Study.pdf', 'page': 10})]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.invoke({\"query\": \"Представь несколько идей по использованию RAG системы\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9418229",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "giga_kernel",
   "language": "python",
   "name": "giga_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
