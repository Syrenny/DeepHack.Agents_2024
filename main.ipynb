{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e7bb5503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./2404.15626v1.An_Electromagnetism_Inspired_Method_for_Estimating_In_Grasp_Torque_from_Visuotactile_Sensors.pdf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "\n",
    "# Construct the default API client.\n",
    "client = arxiv.Client()\n",
    "\n",
    "search = arxiv.Search(\n",
    "    query = \"Attention is all you need\",\n",
    "    max_results = 10,\n",
    "    sort_by=arxiv.SortCriterion.SubmittedDate, \n",
    "    sort_order=arxiv.SortOrder.Descending\n",
    ")\n",
    "\n",
    "results = client.results(search)\n",
    "# `results` is a generator; you can iterate over its elements one by one...\n",
    "for r in client.results(search):\n",
    "    print(r.download_pdf())\n",
    "    print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825786fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"assets/arxiv_paper.pdf\")\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d5ee51",
   "metadata": {},
   "source": [
    "## Инициализация модели\n",
    "Теперь инициализируем модель GigaChat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c762ee3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"credentials.txt\", 'r', encoding='utf-8') as file:\n",
    "    credentials = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ae629e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models.gigachat import GigaChat\n",
    "\n",
    "llm = GigaChat(credentials=credentials, scope=\"GIGACHAT_API_CORP\", verify_ssl_certs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9946de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "loader = TextLoader(\"assets/arxiv_paper.pdf\")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "documents = text_splitter.split_documents(documents)\n",
    "print(f\"Total documents: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0334c57c",
   "metadata": {},
   "source": [
    "После нарезки мы получили 91 документ частями книги.\n",
    "\n",
    "## Создание базы данных эмбеддингов\n",
    "\n",
    "Эмбеддинг это векторное представление текста, которое может быть использовано для определения смысловой близости текстов. Векторная база данных хранит тексты и соответствующие им эмбеддинги, а также умеет выполнять поиск по ним. Для работы с базой данных мы создаем объект GigaChatEmbeddings и передаем его в базу данных Chroma.\n",
    "\n",
    "> Обратите внимание, что сервис для вычисления эмбеддингов может тарифицироваться отдельно от стоимости модели GigaChat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "91b2d538",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.config import Settings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.embeddings.gigachat import GigaChatEmbeddings\n",
    "\n",
    "embeddings = GigaChatEmbeddings(\n",
    "    credentials=credentials, scope=\"GIGACHAT_API_CORP\", verify_ssl_certs=False\n",
    ")\n",
    "\n",
    "db = Chroma.from_documents(\n",
    "    documents,\n",
    "    embeddings,\n",
    "    client_settings=Settings(anonymized_telemetry=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d75064c",
   "metadata": {},
   "source": [
    "## Поиск по базе данных\n",
    "\n",
    "Теперь можно обратиться к базе данных и попросить найти документы, которые с наибольшей вероятностью содержат ответ на наш вопрос.\n",
    "\n",
    "По-умолчанию база данных возвращает 4 наиболее релевантных документа. Этот параметр можно изменить в зависимости от решаемой задачи и типа документов.\n",
    "\n",
    "Видно, что первый же документ содержит внутри себя часть книги с ответом на наш вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1e01260c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = db.similarity_search(question, k=4)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "389ae352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... ение (fork) открытой библиотеки LangСhain на Python. Её главная цель — облегчить жизнь разработчику. Библиотека состоит из большого количества различных компонентов, которые позвол ...\n"
     ]
    }
   ],
   "source": [
    "print(f\"... {str(docs[0])[620:800]} ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e775d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "\n",
    "propmpt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate(\n",
    "            #             prompt = \"\"\"Сгенерируй от {dataset_size_min} до {dataset_size_max} синонимов для слова \"{subject}\".\n",
    "            # Результат верни в формате JSON списка без каких либо пояснений, например, [\"синоним1\", \"синоним2\", \"синоним3\", \"синоним4\"].\n",
    "            # Не дублируй фразы.\"\"\"\n",
    "            prompt=load_prompt(\"lc://prompts/synonyms/synonyms_generation.yaml\")\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18271b89",
   "metadata": {},
   "source": [
    "## QnA цепочка\n",
    "\n",
    "Теперь мы создадим цепочку QnA, которая специально предназначена для ответов на вопросы по документам. В качестве аргументов есть передается языковая модель и ретривер (база данных)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fabc3185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm, retriever=db.as_retriever())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f58a20d",
   "metadata": {},
   "source": [
    "Наконец можно задать вопрос нашей цепочке и получить правильный ответ!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "abda1977",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/Syrenny/GigaHack/Syrenny/giga_venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Какой Loss использует Yolov8?',\n",
       " 'result': 'Я не знаю ответа на этот вопрос.'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdb4c8b",
   "metadata": {},
   "source": [
    "Несколько дополнительных вопросов для проверки работоспособности:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "deb4e065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Расскажи про тимлида Сбера',\n",
       " 'result': 'Извините, но у меня нет информации о конкретном тимлиде Сбера. Могу рассказать вам о том, что такое тимлид и какие у него обязанности. Тимлид - это лидер команды, который отвечает за управление командой, ее развитие и достижение поставленных целей. Он координирует работу членов команды, помогает им решать возникающие проблемы, поддерживает мотивацию и вовлеченность. Кроме того, тимлид также участвует в принятии решений, связанных с проектами, и помогает команде в достижении общих целей.'}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain({\"query\": \"Расскажи про тимлида Сбера\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79c7cd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Основные особенности Yolov8',\n",
       " 'result': 'YOLOv8 имеет несколько особенностей, которые отличают его от других версий YOLO. Во-первых, он использует новую архитектуру, которая сочетает в себе модули FAN и PAN. Это позволяет ему лучше захватывать особенности на разных масштабах и разрешениях, что важно для точного обнаружения объектов разного размера и формы. Кроме того, YOLOv8 превосходит YOLOv5 по нескольким параметрам, включая более высокую метрику mAP и меньшее количество выбросов при измерении против RF100. Он также превосходит YOLOv5 для каждого RF100 категории.'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain({\"query\": \"Основные особенности Yolov8\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b17fcf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import os\n",
    "import json\n",
    "\n",
    "def download_from_arxiv(key_word, max_results=10, destination_path=\"assets/arxiv/\", saved_list_path=\"assets/arxiv/papers.json\"):\n",
    "    client = arxiv.Client()\n",
    "\n",
    "    search = arxiv.Search(\n",
    "        query = str(key_word),\n",
    "        max_results = max_results,\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate, \n",
    "        sort_order=arxiv.SortOrder.Descending\n",
    "    )\n",
    "    # Проверка существования файла с сохраненным списком скачанных arXiv ID\n",
    "    if os.path.exists(saved_list_path):\n",
    "        with open(saved_list_path, \"r\") as f:\n",
    "            downloaded_arxiv_ids = json.load(f)\n",
    "    else:\n",
    "        downloaded_arxiv_ids = []\n",
    "    \n",
    "    for result in client.results(search):\n",
    "        # Проверка наличия статьи в списке уже скачанных\n",
    "        if result.entry_id.split(\"/\")[-1] in downloaded_arxiv_ids:\n",
    "            print(f\"Статья {result.entry_id} уже скачана и пропущена.\")\n",
    "            continue\n",
    "        \n",
    "        # Скачивание PDF-файла статьи\n",
    "        pdf_path = result.download_pdf(dirpath=destination_path)\n",
    "        if pdf_path:\n",
    "            print(f\"Статья {result.entry_id} успешно скачана и сохранена в {pdf_path}\")\n",
    "            # Добавление arXiv ID в список скачанных\n",
    "            downloaded_arxiv_ids.append(result.entry_id.split(\"/\")[-1])\n",
    "    # Сохранение списка скачанных arXiv ID в файл JSON\n",
    "    with open(saved_list_path, \"w\") as f:\n",
    "        json.dump(downloaded_arxiv_ids, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "177ab54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статья http://arxiv.org/abs/2404.15238v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15104v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.14977v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.14963v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.14943v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15382v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.14809v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.14740v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.14695v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.14631v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15488v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.14043v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.13948v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.13892v2 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.13781v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.12879v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.12772v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.12560v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.12457v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.12309v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.14901v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.13957v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.13764v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.13633v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.12926v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.12309v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.09577v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15351v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.09375v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.13066v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15604v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15592v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15588v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15578v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15549v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15522v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15515v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15488v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15485v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15458v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15615v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15608v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15604v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15603v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15592v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15591v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15580v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15576v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15564v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.15552v1 уже скачана и пропущена.\n",
      "Статья http://arxiv.org/abs/2404.06751v1 успешно скачана и сохранена в assets/arxiv/2404.06751v1.Leveraging_open_source_models_for_legal_language_modeling_and_analysis__a_case_study_on_the_Indian_constitution.pdf\n",
      "Статья http://arxiv.org/abs/2403.15729v2 успешно скачана и сохранена в assets/arxiv/2403.15729v2.Towards_a_RAG_based_Summarization_Agent_for_the_Electron_Ion_Collider.pdf\n",
      "Статья http://arxiv.org/abs/2403.14258v1 успешно скачана и сохранена в assets/arxiv/2403.14258v1.LLM_based_Extraction_of_Contradictions_from_Patents.pdf\n",
      "Статья http://arxiv.org/abs/2403.14702v1 успешно скачана и сохранена в assets/arxiv/2403.14702v1.Large_language_model_powered_chatbots_for_internationalizing_student_support_in_higher_education.pdf\n",
      "Статья http://arxiv.org/abs/2403.10482v2 успешно скачана и сохранена в assets/arxiv/2403.10482v2.Can_a_GPT4_Powered_AI_Agent_Be_a_Good_Enough_Performance_Attribution_Analyst_.pdf\n",
      "Статья http://arxiv.org/abs/2403.10588v1 успешно скачана и сохранена в assets/arxiv/2403.10588v1.S3LLM__Large_Scale_Scientific_Software_Understanding_with_LLMs_using_Source__Metadata__and_Document.pdf\n",
      "Статья http://arxiv.org/abs/2403.00830v1 успешно скачана и сохранена в assets/arxiv/2403.00830v1.MedAide__Leveraging_Large_Language_Models_for_On_Premise_Medical_Assistance_on_Edge_Devices.pdf\n",
      "Статья http://arxiv.org/abs/2403.05568v1 успешно скачана и сохранена в assets/arxiv/2403.05568v1.Revolutionizing_Mental_Health_Care_through_LangChain__A_Journey_with_a_Large_Language_Model.pdf\n",
      "Статья http://arxiv.org/abs/2402.06929v1 успешно скачана и сохранена в assets/arxiv/2402.06929v1.Making_a_prototype_of_Seoul_historical_sites_chatbot_using_Langchain.pdf\n",
      "Статья http://arxiv.org/abs/2402.01733v1 успешно скачана и сохранена в assets/arxiv/2402.01733v1.Development_and_Testing_of_Retrieval_Augmented_Generation_in_Large_Language_Models____A_Case_Study_Report.pdf\n"
     ]
    }
   ],
   "source": [
    "key_words = [\"NLP\", \"RAG\", \"ChatBot\", \"LLM\", \"Speech Recognition\", \"LangChain\", \"LLM Agents\"]\n",
    "\n",
    "for key_word in key_words:\n",
    "    # Пример использования функции\n",
    "    download_from_arxiv(key_word, max_results=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "947ab4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_files(directory):\n",
    "    pdf_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".pdf\"):\n",
    "                pdf_files.append(os.path.join(root, file))\n",
    "    return pdf_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e6088a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assets/arxiv/2404.15604v1.Hybrid_LLM_Rule_based_Approaches_to_Business_Insights_Generation_from_Structured_Data.pdf\n",
      "assets/arxiv/2404.15588v1.Minimal_Evidence_Group_Identification_for_Claim_Verification.pdf\n",
      "assets/arxiv/2404.15578v1.Can_Foundational_Large_Language_Models_Assist_with_Conducting_Pharmaceuticals_Manufacturing_Investigations_.pdf\n",
      "assets/arxiv/2404.15564v1.Guided_AbsoluteGrad__Magnitude_of_Gradients_Matters_to_Explanation_s_Localization_and_Saliency.pdf\n",
      "assets/arxiv/2404.15592v1.ImplicitAVE__An_Open_Source_Dataset_and_Multimodal_LLMs_Benchmark_for_Implicit_Attribute_Value_Extraction.pdf\n",
      "assets/arxiv/2404.13892v2.Retrieval_Augmented_Audio_Deepfake_Detection.pdf\n",
      "assets/arxiv/2404.14631v1.Learning_Word_Embedding_with_Better_Distance_Weighting_and_Window_Size_Scheduling.pdf\n",
      "assets/arxiv/2404.15552v1.Cross_Temporal_Spectrogram_Autoencoder__CTSAE___Unsupervised_Dimensionality_Reduction_for_Clustering_Gravitational_Wave_Glitches.pdf\n",
      "assets/arxiv/2404.14901v1.Beyond_Code_Generation__An_Observational_Study_of_ChatGPT_Usage_in_Software_Engineering_Practice.pdf\n",
      "assets/arxiv/2404.15615v1.MDDD__Manifold_based_Domain_Adaptation_with_Dynamic_Distribution_for_Non_Deep_Transfer_Learning_in_Cross_subject_and_Cross_session_EEG_based_Emotion_Recognition.pdf\n",
      "assets/arxiv/2404.13781v1.Evaluating_Retrieval_Quality_in_Retrieval_Augmented_Generation.pdf\n",
      "assets/arxiv/2404.09375v1.Deceptive_Patterns_of_Intelligent_and_Interactive_Writing_Assistants.pdf\n",
      "assets/arxiv/2404.12560v1.Dubo_SQL__Diverse_Retrieval_Augmented_Generation_and_Fine_Tuning_for_Text_to_SQL.pdf\n",
      "assets/arxiv/2404.15382v1.Feature_Distribution_Shift_Mitigation_with_Contrastive_Pretraining_for_Intrusion_Detection.pdf\n",
      "assets/arxiv/2404.14740v1.Modeling_the_Sacred__Considerations_when_Using_Considerations_when_Using_Religious_Texts_in_Natural_Language_Processing.pdf\n",
      "assets/arxiv/2404.14943v1.Does_It_Make_Sense_to_Explain_a_Black_Box_With_Another_Black_Box_.pdf\n",
      "assets/arxiv/2404.12457v1.RAGCache__Efficient_Knowledge_Caching_for_Retrieval_Augmented_Generation.pdf\n",
      "assets/arxiv/2404.15608v1.Understanding_and_Improving_CNNs_with_Complex_Structure_Tensor__A_Biometrics_Study.pdf\n",
      "assets/arxiv/2404.09577v1.Transformers__Contextualism__and_Polysemy.pdf\n",
      "assets/arxiv/2404.15591v1.Domain_Adaptation_for_Learned_Image_Compression_with_Supervised_Adapters.pdf\n",
      "assets/arxiv/2404.15522v1.Towards_Systematic_Evaluation_of_Logical_Reasoning_Ability_of_Large_Language_Models.pdf\n",
      "assets/arxiv/2404.13066v1.Leveraging_Large_Language_Model_as_Simulated_Patients_for_Clinical_Education.pdf\n",
      "assets/arxiv/2404.13764v1.Using_Adaptive_Empathetic_Responses_for_Teaching_English.pdf\n",
      "assets/arxiv/2404.15603v1.Development_of_Pattern_Recognition_Validation_for_Boson_Sampling.pdf\n",
      "assets/arxiv/2404.14695v1.MisgenderMender__A_Community_Informed_Approach_to_Interventions_for_Misgendering.pdf\n",
      "assets/arxiv/2404.15458v1.Can_Large_Language_Models_Learn_the_Physics_of_Metamaterials__An_Empirical_Study_with_ChatGPT.pdf\n",
      "assets/arxiv/2404.14043v1.LLMs_Know_What_They_Need__Leveraging_a_Missing_Information_Guided_Framework_to_Empower_Retrieval_Augmented_Generation.pdf\n",
      "assets/arxiv/2404.15238v1.CultureBank__An_Online_Community_Driven_Knowledge_Base_Towards_Culturally_Aware_Language_Technologies.pdf\n",
      "assets/arxiv/2404.15580v1.MiM__Mask_in_Mask_Self_Supervised_Pre_Training_for_3D_Medical_Image_Analysis.pdf\n",
      "assets/arxiv/2404.15576v1.Designing_AI_Enabled_Games_to_Support_Social_Emotional_Learning_for_Children_with_Autism_Spectrum_Disorders.pdf\n",
      "assets/arxiv/2404.12926v1.MM_PhyRLHF__Reinforcement_Learning_Framework_for_Multimodal_Physics_Question_Answering.pdf\n",
      "assets/arxiv/2404.12879v1.Unlocking_Multi_View_Insights_in_Knowledge_Dense_Retrieval_Augmented_Generation.pdf\n",
      "assets/arxiv/2404.15485v1.Large_Language_Models_Spot_Phishing_Emails_with_Surprising_Accuracy__A_Comparative_Analysis_of_Performance.pdf\n",
      "assets/arxiv/2404.15549v1.PRISM__Patient_Records_Interpretation_for_Semantic_Clinical_Trial_Matching_using_Large_Language_Models.pdf\n",
      "assets/arxiv/2404.12772v1.Generating_Test_Scenarios_from_NL_Requirements_using_Retrieval_Augmented_LLMs__An_Industrial_Study.pdf\n",
      "assets/arxiv/2404.14977v1.Social_Media_and_Artificial_Intelligence_for_Sustainable_Cities_and_Societies__A_Water_Quality_Analysis_Use_case.pdf\n",
      "assets/arxiv/2404.14963v1.Achieving__97__on_GSM8K__Deeply_Understanding_the_Problems_Makes_LLMs_Perfect_Reasoners.pdf\n",
      "assets/arxiv/2404.15351v1.Integrating_Physiological_Data_with_Large_Language_Models_for_Empathic_Human_AI_Interaction.pdf\n",
      "assets/arxiv/2404.13957v1.How_Well_Can_LLMs_Echo_Us__Evaluating_AI_Chatbots__Role_Play_Ability_with_ECHO.pdf\n",
      "assets/arxiv/2404.15104v1.Identifying_Fairness_Issues_in_Automatically_Generated_Testing_Content.pdf\n",
      "assets/arxiv/2404.12309v1.iRAG__An_Incremental_Retrieval_Augmented_Generation_System_for_Videos.pdf\n",
      "assets/arxiv/2404.14809v1.A_Survey_of_Large_Language_Models_on_Generative_Graph_Analytics__Query__Learning__and_Applications.pdf\n",
      "assets/arxiv/2404.13948v1.Typos_that_Broke_the_RAG_s_Back__Genetic_Attack_on_RAG_Pipeline_by_Simulating_Documents_in_the_Wild_via_Low_level_Perturbations.pdf\n",
      "assets/arxiv/2404.15515v1.ToM_LM__Delegating_Theory_Of_Mind_Reasoning_to_External_Symbolic_Executors_in_Large_Language_Models.pdf\n",
      "assets/arxiv/2404.15488v1.IryoNLP_at_MEDIQA_CORR_2024__Tackling_the_Medical_Error_Detection___Correction_Task_On_the_Shoulders_of_Medical_Agents.pdf\n",
      "assets/arxiv/2404.13633v1.Incorporating_Different_Verbal_Cues_to_Improve_Text_Based_Computer_Delivered_Health_Messaging.pdf\n"
     ]
    }
   ],
   "source": [
    "# Пример использования функции\n",
    "directory = \"assets/arxiv/\"\n",
    "pdf_files = get_pdf_files(directory)\n",
    "for pdf_file in pdf_files:\n",
    "    print(pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2d47f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "\n",
    "def summarize(path_to_pdf_file, chunk_size=5000, chunk_overlap=1000): \n",
    "    loader = PyMuPDFLoader(path_to_pdf_file)\n",
    "    docs = loader.load()\n",
    "    split_docs = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap).split_documents(\n",
    "        docs\n",
    "    )\n",
    "    \n",
    "    giga = GigaChat(credentials=credentials, scope=\"GIGACHAT_API_CORP\", verify_ssl_certs=False)\n",
    "    chain = load_summarize_chain(giga, chain_type=\"map_reduce\")\n",
    "    res = chain.run(split_docs)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "47a6448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "class ArticlesDB:\n",
    "    def __init__(self, db_path='articles.db'):\n",
    "        self.conn = sqlite3.connect(db_path)\n",
    "        self.cursor = self.conn.cursor()\n",
    "        self.create_table()\n",
    "\n",
    "    def create_table(self):\n",
    "        self.cursor.execute('''CREATE TABLE IF NOT EXISTS articles\n",
    "                            (id INTEGER PRIMARY KEY,\n",
    "                            title TEXT NOT NULL,\n",
    "                            summary TEXT)''')\n",
    "        self.conn.commit()\n",
    "\n",
    "    def add_article(self, title, summary):\n",
    "        self.cursor.execute('INSERT INTO articles (title, summary) VALUES (?, ?)', (title, summary))\n",
    "        self.conn.commit()\n",
    "\n",
    "    def delete_article(self, title):\n",
    "        self.cursor.execute('DELETE FROM articles WHERE title = ?', (title,))\n",
    "        self.conn.commit()\n",
    "\n",
    "    def get_article_summary(self, title):\n",
    "        self.cursor.execute('SELECT summary FROM articles WHERE title = ?', (title,))\n",
    "        result = self.cursor.fetchone()\n",
    "        if result:\n",
    "            return result[0]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def get_all_articles(self):\n",
    "        self.cursor.execute('SELECT title, summary FROM articles')\n",
    "        return self.cursor.fetchall()\n",
    "\n",
    "    def close(self):\n",
    "        self.conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e84e7200",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_db = ArticlesDB(db_path=\"articles.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a551cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for filename in tqdm(pdf_files, desc=\"Processing PDF files\"):\n",
    "    article_title = filename.split(\"/\")[-1][:-4]\n",
    "    article_summary = articles_db.get_article_summary(article_title)\n",
    "    if article_summary is None:\n",
    "        result = summarize(filename,\n",
    "                       chunk_size=5000,\n",
    "                       chunk_overlap=1000)\n",
    "        articles_db.add_article(article_title, result)\n",
    "        print(f\"Article \\\"{article_title}\\\" saved to database\")\n",
    "print(articles_db.get_all_articles())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "bf85958a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2404.15604v1.Hybrid_LLM_Rule_based_Approaches_to_Business_Insights_Generation_from_Structured_Data\n",
      "The paper discusses the integration of rule-based systems and Large Language Models (LLMs) to generate actionable business insights from structured data. The hybrid approach combines the robustness of rule-based systems with the adaptive power of LLMs, addressing the complexities of data extraction in the field of business intelligence. The hybrid approach combines interpretable AI techniques, such as LIME, rule-based systems, and supervised document classification, to extract actionable insights from business data. LLM plays a vital role in this process by enhancing the understanding and generation of natural language-generated insights. The hybrid approach can uncover personalized and nuanced user interests, needs, and goals from user journeys and user activities on the platform. However, when implementing this approach, it is essential to consider data quality, domain knowledge, and scalability. Additionally, there are ethical concerns related to the use of large language models, as they can perpetuate biases and misinformation present in the training data.\n",
      "\n",
      "2404.15588v1.Minimal_Evidence_Group_Identification_for_Claim_Verification\n",
      "This paper presents a method for identifying minimal evidence groups (MEGs) in claim verification. The proposed approach significantly outperforms baseline methods on precision and F0.5 scores. The non-redundancy of MEGs improves the performance of downstream applications.\n",
      "\n",
      "2404.15578v1.Can_Foundational_Large_Language_Models_Assist_with_Conducting_Pharmaceuticals_Manufacturing_Investigations_\n",
      "This paper explores the potential of large language models (LLMs) in assisting with pharmaceutical manufacturing investigations. LLMs can extract specific information from unstructured data and identify similar or related deviations through semantic search on historical records. GPT-4 and Claude-2 show high accuracy in information extraction and similarity identification. However, the complex interplay between reasoning and hallucination behavior is a risk factor. Incidents in a manufacturing environment can lead to production halts, safety or environmental issues, and product quality concerns. Investigations aim to identify main causes, potential impacts, and preventive actions. Organizations store knowledge from previous incidents in two forms: tactic knowledge and formal records. Automated approaches enabled by IT capabilities, databases, and AI and data analytics tools can address incident records and lessons learned. The potential use of Natural Language Processing (NLP) techniques for automation, particularly with the introduction of Large Language Models (LLMs), is discussed. However, the challenges and barriers associated with the application of LLMs in industries with high accuracy standards, such as manufacturing, are also highlighted.\n",
      "\n",
      "2404.15564v1.Guided_AbsoluteGrad__Magnitude_of_Gradients_Matters_to_Explanation_s_Localization_and_Saliency\n",
      "This paper presents a new gradient-based XAI method called Guided AbsoluteGrad for saliency map explanations. The method utilizes both positive and negative gradient magnitudes and employs gradient variance to distinguish the important areas for noise deduction. It also introduces a novel evaluation metric named ReCover And Predict (RCAP), which considers the Localization and Visual Noise Level objectives of the explanations. The method is evaluated with seven gradient-based XAI methods using the RCAP metric and other SOTA metrics in three case studies: ImageNet dataset with ResNet50 model, International Skin Imaging Collaboration (ISIC) dataset with EfficientNet model, and Places365 dataset with DenseNet161 model. The method surpasses other gradient-based approaches, showcasing the quality of enhanced saliency map explanations through gradient magnitude.\n",
      "\n",
      "2404.15592v1.ImplicitAVE__An_Open_Source_Dataset_and_Multimodal_LLMs_Benchmark_for_Implicit_Attribute_Value_Extraction\n",
      "This paper presents ImplicitAVE, a publicly available multimodal dataset and benchmark for implicit attribute value extraction. The dataset addresses limitations of existing AVE datasets, such as focusing on explicit attribute values, lacking product images, and being limited to a few domains. ImplicitAVE includes 68,000 training and 1,600 testing data points across five domains, and it explores the application of multimodal large language models (MLLMs) for implicit AVE. The authors evaluate six recent MLLMs with eleven variants, revealing that implicit value extraction remains a challenging task for MLLMs.\n",
      "\n",
      "2404.13892v2.Retrieval_Augmented_Audio_Deepfake_Detection\n",
      "The paper presents a retrieval-augmented detection (RAD) framework for audio deepfake detection. The framework uses retrieval-augmented generation (RAG) to enhance detection by augmenting test samples with similar retrieved samples. The multi-fusion attentive classifier is also extended to integrate it with the proposed RAD framework. Experimental results show the superior performance of the proposed RAD framework over baseline methods, achieving state-of-the-art results on the ASVspoof 2021 DF set and competitive results on the 2019 and 2021 LA sets.\n",
      "\n",
      "2404.14631v1.Learning_Word_Embedding_with_Better_Distance_Weighting_and_Window_Size_Scheduling\n",
      "This paper proposes two novel methods, Learnable Formulated Weights (LFW) and Epoch-based Dynamic Window Size (EDWS), to incorporate distance information into two variants of Word2Vec, the Continuous Bag-of-Words (CBOW) model and the Continuous Skip-gram (Skip-gram) model. LFW uses a formula with learnable parameters to calculate distance-related weights for average pooling in CBOW, while EDWS improves the dynamic window size strategy in Skip-gram to introduce distance information in a more balanced way. Experiments show the effectiveness of these methods in enhancing Word2Vec's performance, surpassing previous state-of-the-art methods.\n",
      "\n",
      "2404.15552v1.Cross_Temporal_Spectrogram_Autoencoder__CTSAE___Unsupervised_Dimensionality_Reduction_for_Clustering_Gravitational_Wave_Glitches\n",
      "The Cross-Temporal Spectrogram Autoencoder (CTSAE) is an unsupervised method for dimensionality reduction and clustering of gravitational wave glitches. It integrates a four-branch autoencoder with convolutional neural networks and vision transformers. CTSAE was trained and evaluated on the GravitySpy O3 dataset, and demonstrated superior performance in clustering tasks compared to state-of-the-art semi-supervised learning methods. This pioneering unsupervised approach marks a significant step forward in the field of gravitational wave research.\n",
      "\n",
      "2404.14901v1.Beyond_Code_Generation__An_Observational_Study_of_ChatGPT_Usage_in_Software_Engineering_Practice\n",
      "This paper presents an observational study of professional software engineers' experiences with ChatGPT, a large language model, in their work. The study examines how practitioners use ChatGPT and the factors that influence their experience with the tool. The authors find that practitioners often use ChatGPT for guidance and learning rather than generating ready-to-use software artifacts. They also propose a theoretical framework for understanding the interaction between users, internal factors, and external factors in shaping the experience with ChatGPT.\n",
      "\n",
      "2404.15615v1.MDDD__Manifold_based_Domain_Adaptation_with_Dynamic_Distribution_for_Non_Deep_Transfer_Learning_in_Cross_subject_and_Cross_session_EEG_based_Emotion_Recognition\n",
      "The paper presents a novel non-deep transfer learning method called Manifold-based Domain Adaptation with Dynamic Distribution (MDDD) for cross-subject and cross-session EEG-based emotion recognition. The proposed MDDD model includes four main modules: manifold feature transformation, dynamic distribution alignment, classifier learning, and ensemble learning. The data is transformed onto an optimal Grassmann manifold space, allowing for dynamic alignment of the source and target domains. The classifier learning integrates the principle of structural risk minimization to develop robust classification models, while the ensemble learning module aggregates the classifiers obtained at different stages of the optimization process to enhance prediction accuracy. The MDDD model is validated with two well-known databases and two different validation methods (cross-subject single-session and cross-subject cross-session), demonstrating improved performance compared to traditional non-deep learning methods and comparable performance to deep learning methods.\n",
      "\n",
      "2404.13781v1.Evaluating_Retrieval_Quality_in_Retrieval_Augmented_Generation\n",
      "This paper presents eRAG, a novel evaluation approach for retrieval-augmented generation (RAG) systems. Instead of using traditional end-to-end evaluation methods, eRAG utilizes the large language model (LLM) within the RAG system to generate output for each document in the retrieval list. The generated output is then evaluated based on the downstream task ground truth labels. The downstream performance for each document serves as its relevance label. Various downstream task metrics are employed to obtain document-level annotations and aggregated using set-based or ranking metrics. Experiments on a wide range of datasets demonstrate that eRAG achieves a higher correlation with downstream RAG performance compared to baseline methods, with improvements in Kendall's τ correlation ranging from 0.168 to 0.494. Additionally, eRAG offers significant computational advantages, improving runtime and consuming up to 50 times less GPU memory than end-to-end evaluation.\n",
      "\n",
      "2404.09375v1.Deceptive_Patterns_of_Intelligent_and_Interactive_Writing_Assistants\n",
      "This paper discusses the potential use of deceptive user interface/user experience (UI/UX) patterns in interactive and intelligent writing assistants like ChatGPT and systems that help with text-related tasks. The authors aim to raise awareness about these patterns and encourage further research into how the design and interaction of such systems can affect users and their writing. The paper presents a set of deceptive UI/UX patterns for writing assistants, including manipulating the user interface to privilege certain actions, forcing users to perform certain actions, and hiding costs and charges. The potential motivation for these patterns is opinion influence and financial gains. The paper highlights the need for longitudinal user studies to understand the potential consequences of these patterns on user dependency and skill development.\n",
      "\n",
      "2404.12560v1.Dubo_SQL__Diverse_Retrieval_Augmented_Generation_and_Fine_Tuning_for_Text_to_SQL\n",
      "Dubo-SQL is a text-to-SQL method that improves execution accuracy on the BIRD-SQL benchmark. Dubo-SQL v1 uses GPT-3.5 Turbo and exceeds the performance of the next-best model using OpenAI. Dubo-SQL v2 uses GPT-4 Turbo and RAG to push EX higher. Dubo-SQL v1 costs $273 at OpenAI's pricing, while Dubo-SQL v2 costs under $0.14 per question.\n",
      "\n",
      "2404.15382v1.Feature_Distribution_Shift_Mitigation_with_Contrastive_Pretraining_for_Intrusion_Detection\n",
      "This paper presents SwapCon, a machine learning model designed to address the feature distribution shift problem in network intrusion detection. The model is pretrained using contrastive learning and swapping augmentation strategies to learn general patterns from a large dataset. The authors demonstrate that pretraining can increase robustness against feature distribution shift by over 8% and show that the proposed SwapCon model outperforms other models like eXtreme Gradient Boosting (XGBoost) and K-Nearest Neighbor (KNN) by a large margin.\n",
      "\n",
      "2404.14740v1.Modeling_the_Sacred__Considerations_when_Using_Considerations_when_Using_Religious_Texts_in_Natural_Language_Processing\n",
      "This paper discusses the use of religious texts in natural language processing (NLP) and its ethical implications. It argues that NLP's use of such texts raises considerations beyond model biases, including data provenance, cultural contexts, and their use in proselytism. The paper calls for more consideration of researcher positionality and the perspectives of marginalized linguistic and religious communities.\n",
      "\n",
      "2404.14943v1.Does_It_Make_Sense_to_Explain_a_Black_Box_With_Another_Black_Box_\n",
      "The paper compares transparent and opaque counterfactual explanation methods in NLP tasks. Transparent methods perturb the target document by adding, removing, or replacing words, while opaque approaches project the target document into a latent, non-interpretable space. The authors' empirical evidence suggests that opaque approaches may be overkill for downstream applications like fake news detection or sentiment analysis, as they add an additional level of complexity without significant performance gain. This observation raises the question of whether it makes sense to explain a black box with another black box.\n",
      "\n",
      "2404.12457v1.RAGCache__Efficient_Knowledge_Caching_for_Retrieval_Augmented_Generation\n",
      "The text discusses a novel multilevel dynamic caching system called RAGCache, designed for efficient knowledge caching in retrieval-augmented generation. RAGCache aims to reduce the computation and memory costs associated with long sequence generation in RAG systems. The system organizes the intermediate states of retrieved knowledge in a knowledge tree and caches them in the GPU and host memory hierarchy. It proposes a replacement policy that is aware of LLM inference characteristics and RAG retrieval patterns, and dynamically overlaps the retrieval and inference steps to minimize the end-to-end latency. Experimental results show that RAGCache reduces the time to first token by up to 4x and improves the throughput by up to 2.1x compared to vLLM integrated with Faiss.\n",
      "\n",
      "2404.15608v1.Understanding_and_Improving_CNNs_with_Complex_Structure_Tensor__A_Biometrics_Study\n",
      "The paper presents a study that demonstrates the effectiveness of using Complex Structure Tensor as an input to Convolutional Neural Networks (CNNs) for biometric identification. This approach improves identification accuracy compared to using grayscale inputs alone. Mini complex conv-nets combined with reduced CNN sizes outperform full-fledged CNN architectures, suggesting that the upfront use of orientation features in CNNs enhances their explainability and relevance to thin-clients. The experiments were conducted on publicly available data sets for biometric identification and verification using 6 State of the Art CNN architectures. The study reduced the Equal Error Rate (EER) on the PolyU dataset by 5-26% depending on data and scenario.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, summary in articles_db.get_all_articles():\n",
    "    print(name)\n",
    "    print(summary)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c8a608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models.gigachat import GigaChat\n",
    "\n",
    "with open(\"credentials.txt\", 'r', encoding='utf-8') as file:\n",
    "    credentials = file.read()\n",
    "    \n",
    "llm = GigaChat(credentials=credentials, scope=\"GIGACHAT_API_CORP\", verify_ssl_certs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce04616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "loader = PyPDFDirectoryLoader(\"assets/arxiv/\")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "documents = text_splitter.split_documents(documents)\n",
    "print(f\"Total documents: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1a847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.config import Settings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.embeddings.gigachat import GigaChatEmbeddings\n",
    "\n",
    "embeddings = GigaChatEmbeddings(\n",
    "    credentials=credentials, scope=\"GIGACHAT_API_CORP\", verify_ssl_certs=False\n",
    ")\n",
    "\n",
    "db = Chroma.from_documents(\n",
    "    documents,\n",
    "    embeddings,\n",
    "    client_settings=Settings(anonymized_telemetry=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce918b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm, retriever=db.as_retriever())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "giga_kernel",
   "language": "python",
   "name": "giga_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
