{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dec9a520",
   "metadata": {},
   "source": [
    "![Use case 1](../../static/img/summarization_use_case_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c82979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"credentials.txt\", 'r', encoding='utf-8') as file:\n",
    "    credentials = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d49425e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parts count: 7\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.chat_models.gigachat import GigaChat\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# loader = TextLoader(\"assets/arxiv_paper.pdf\")\n",
    "# docs = loader.load()\n",
    "\n",
    "loader = PyPDFLoader(\"assets/arxiv_paper.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# loader = WebBaseLoader(\"https://habr.com/ru/companies/sberdevices/articles/794773/\")\n",
    "# docs = loader.load()\n",
    "\n",
    "split_docs = CharacterTextSplitter(chunk_size=5000, chunk_overlap=1000).split_documents(\n",
    "    docs\n",
    ")\n",
    "print(f\"Parts count: {len(split_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037e299a",
   "metadata": {},
   "source": [
    "Далее с помощью `GigaChat`` выполняется суммаризация каждой части (фаза map). В конце все суммаризированные части объединяются в одну, после чего выполняется суммарная суммаризация."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b1f51a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===\n",
      "This paper presents a method for estimating in-grasp torque from visuotactile sensors using the concept of the Tactile Dipole Moment. The method does not rely on deep learning or sensor-specific modeling, but instead takes inspiration from electromagnetism to analyze the vector field produced from 2D marker displacements. Experiments with a real robot demonstrate the accuracy of the method in measuring tilt torques during a USB stick insertion task. The method is applicable to other visuotactile sensing hardware and grasped object shapes. The authors open source the implementation of their algorithm and experimental apparatus design files.\n"
     ]
    }
   ],
   "source": [
    "giga = GigaChat(credentials=credentials, scope=\"GIGACHAT_API_CORP\", verify_ssl_certs=False)\n",
    "chain = load_summarize_chain(giga, chain_type=\"map_reduce\")\n",
    "res = chain.run(split_docs)\n",
    "\n",
    "print(\"\\n\\n===\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14073ed",
   "metadata": {},
   "source": [
    "### Погружаемся глубже\n",
    " \n",
    "**Кастомизация** \n",
    "\n",
    "* Вы можете самостоятельно настроить промпты для каждой фазы map-reduce.\n",
    "\n",
    "**Реальные примеры использования**\n",
    "\n",
    "* См. [this blog post](https://blog.langchain.dev/llms-to-improve-documentation/) кейс по анализу взаимодействия с пользователем (вопросы по документации LangChain)!  \n",
    "* Данном посте [repo](https://github.com/mendableai/QA_clustering) представлена кластеризация как средство суммаризации.\n",
    "* Это открывает третий путь, помимо подходов «stuff» или «map-reduce», который стоит рассмотреть.\n",
    "\n",
    "![Image description](../../static/img/summarization_use_case_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9e13cb",
   "metadata": {},
   "source": [
    "## Подход 3. Refine\n",
    " \n",
    "[Refine](/docs/modules/chains/document/refine) подход похож на mad-reduce:\n",
    "\n",
    "> Цепочка refine создает ответ, перебирая входные документы и итеративно обновляя свой ответ. В цикле происходит суммаризация каждого документа с добавлением полученной на предыдущем шаге суммаризации предыдущих документов.\n",
    "\n",
    "Вы можете легко запустить эту цепочку, использовав параметр `chain_type=\"refine\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2d4e1eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GigaChain - это форк открытой библиотеки LangChain на Python, который был создан для упрощения жизни разработчика. Он содержит множество компонентов, позволяющих работать с промптами, объединять вызовы к большим языковым моделям в цепочки, загружать данные из разных источников и сохранять обработанные ответы языковой модели. GigaChain отличается от LangChain тем, что он переведен на русский язык, адаптирован к экосистеме русскоязычных языковых моделей и, конечно же, к GigaChat. Фреймворк совместим со сторонним сервисом LangSmith - платформой для разработчиков, которая позволяет отлаживать, тестировать, оценивать и отслеживать цепочки, построенные на любой платформе LLM. GigaChain содержит большое количество инструментов и примеров, которые можно использовать в качестве основы своего проекта или просто изучать для расширения кругозора в области возможных кейсов применений LLM.\\n\\nRAG (Retrieval Augmented Generation) - это подход к обогащению вопроса пользователя дополнительными данными, чтобы помочь сети дать правильный ответ. Этот подход включает в себя хранение, поиск дополнительной релевантной информации и ее интеграцию в контекст модели. Таким образом, документы, содержащие актуальную для запроса информацию, будут вставлены в контекст модели перед началом генерации ответа, что позволит учесть специфику доменной области в работе LLM без дообучения.\\n\\nВ GigaChain есть множество готовых пайплайнов с различными реализациями RAG. Вы можете быстро проверить различные комбинации, посмотреть на готовые примеры и собрать ту реализацию RAG, которая лучше всего будет работать с вашими документами и с вашим типом бизнес-задач.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = load_summarize_chain(giga, chain_type=\"refine\")\n",
    "chain.run(split_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68e11a6",
   "metadata": {},
   "source": [
    "It's also possible to supply a prompt and return intermediate steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17f1aa3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/Syrenny/GigaHack/Syrenny/giga_venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "Giga generation stopped with reason: length\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"Write a concise summary of the following:\n",
    "{text}\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "refine_template = (\n",
    "    \"Your job is to produce a final summary\\n\"\n",
    "    \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n",
    "    \"We have the opportunity to refine the existing summary\"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{text}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"Given the new context, refine the original summary in Italian\"\n",
    "    \"If the context isn't useful, return the original summary.\"\n",
    ")\n",
    "refine_prompt = PromptTemplate.from_template(refine_template)\n",
    "chain = load_summarize_chain(\n",
    "    llm=giga,\n",
    "    chain_type=\"refine\",\n",
    "    question_prompt=prompt,\n",
    "    refine_prompt=refine_prompt,\n",
    "    return_intermediate_steps=True,\n",
    "    input_key=\"input_documents\",\n",
    "    output_key=\"output_text\",\n",
    ")\n",
    "result = chain({\"input_documents\": split_docs}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4841618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le nostre opportunità sono di raffinare il sommario esistente con un po' più di contesto sottostante.\n"
     ]
    }
   ],
   "source": [
    "print(result[\"output_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f97140a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GigaChain - это форк открытой библиотеки LangChain на Python, который был создан для упрощения жизни разработчика. Он содержит множество компонентов, позволяющих работать с промптами, объединять вызовы к большим языковым моделям в цепочки, загружать данные из разных источников и сохранять обработанные ответы языковой модели. GigaChain отличается от LangChain тем, что он переведен на русский язык, адаптирован к экосистеме русскоязычных языковых моделей и, конечно же, к GigaChat. Фреймворк совместим со сторонним сервисом LangSmith - платформой для разработчиков, которая позволяет отлаживать, тестировать, оценивать и отслеживать цепочки, построенные на любой платформе LLM.\n",
      "\n",
      "La nostra opportunità è di raffinare il sommario esistente con un po' più di contesto sottostante.\n",
      "\n",
      "------------\n",
      "In questi casi, uno dei metodi è RAG (Retrieval Augmented Generation), che spiegherò in seguito.\n",
      "\n",
      "Risposte alle domande dai documenti (RAG)\n",
      "L'idea di RAG è arricchire la domanda del utente con informazioni aggiuntive che gli permetteranno di dare una risposta corretta. È importante scegliere le informazioni che saranno davvero utili.\n",
      "\n",
      "RAG include l'archiviazione, la ricerca di informazioni aggiuntive rilevanti e la loro integrazione nel contesto della modello. Quindi, i documenti che contengono informazioni rilevanti per la domanda saranno inseriti nel contesto della modello prima dell'inizio della generazione della risposta, permettendo di tenere conto delle caratteristiche specifiche della domanda senza dover riaddestrare la modello.\n",
      "\n",
      "Funziona così: un algoritmo speciale taglia i documenti del utente in piccoli frammenti, dopo di che effettua la ricerca per trovare quelle parti che hanno la maggior probabilità di contenere la risposta alla domanda. Descriverò in seguito la realizzazione di questo soluzione.\n",
      "\n",
      "\n",
      "Per cominciare, utilizziamo diversi adattatori per caricare i documenti del utente nella memoria. Nel contesto di GigaChain, ci sono abbastanza adattatori-caricatori di documenti. Puoi scaricare file PDF, file testuali, puoi scaricare dati da internet, dalla Wikipedia, da altre fonti come la base di dati dei brevetti.\n",
      "\n",
      "Dopo aver caricato i documenti, ogni documento viene tagliato in alcune parti e per ciascuna parte viene calcolato un embedding.\n",
      "\n",
      "L'embedding è un vettore numerico, l'interno rappresentazione del testo da parte della modello. Si è notato che i testi con lo stesso significato hanno embeddings simili. Quindi, questi vettori di significato vengono accumulati in una base di dati vettoriale speciale. Nel contesto di GigaChain, supportiamo le più popolari basi di dati, come Chroma e FAISS.\n",
      "\n",
      "\n",
      "Poi passiamo alla domanda del utente. Dopo che il utente l'ha posta, per la domanda viene calcolato anche un embedding e viene effettuata la ricerca di parti rilevanti nella base di dati. Successivamente, dalle parti rilevanti viene creato un nuovo documento metadato, in modo tale che possa essere inserito nel comando alla modello, e dopo di che la modello può rispondere alla domanda del utente.\n",
      "\n",
      "Come implementare RAG? Il utente che vuole implementarlo si trova di fronte a molte domande:\n",
      "\n",
      "Quali parti tagliare il documento?\n",
      "\n",
      "Qual'è la base di dati vettoriale da utilizzare?\n",
      "\n",
      "Qual'è l'algoritmo di comparazione degli embeddings?\n",
      "\n",
      "Quali trucchi applicare? Ad esempio, si può generare sinonimi della domanda o possibili risposte alternative e poi effettuare la ricerca. Oppure si può arricchire ogni frammento del documento con informazioni aggiuntive, come ad esempio a quale capitolo appartiene, quale era il numero di pagina e così via.\n",
      "\n",
      "Rispondere a queste domande è piuttosto difficile, non esiste una metodologia che consenta di scegliere gli algoritmi ottimali. Di solito la migliore soluzione è un approccio empirico: proviamo diverse implementazioni di RAG finché non troviamo quella ottimale. È molto faticoso fare tutto questo manualmente, e qui di nuovo ci aiuta GigaChain. La libreria contiene molti caricatori di documenti, adattatori per diverse basi di dati, connessioni a diversi algoritmi di ricerca e così via.\n",
      "\n",
      "\n",
      "Ad esempio, a oggi, GigaChain offre 161 caricatori di documenti ereditati da LangChain, 63 archiviazioni vettoriali, 48 modi per calcolare gli embeddings e ogni giorno appaiono nu\n",
      "\n",
      "La nostra opportunità è di raffinare il sommario esistente con un po' più di contesto sottostante.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n\".join(result[\"intermediate_steps\"][:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cfc6db",
   "metadata": {},
   "source": [
    "В случае ниже статья не превышает 1000 токенов, поэтому мы суммаризуем её методом Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5961abcc",
   "metadata": {},
   "source": [
    "В случае ниже, у нас документы превышают 1000 токенов, поэтому мы используем Map-Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7384b1a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "giga_kernel",
   "language": "python",
   "name": "giga_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
